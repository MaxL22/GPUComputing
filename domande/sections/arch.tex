% !TeX spellcheck = it_IT
\section{Architettura}

\begin{questions}
    \question Parallelismo dinamico.
    
    \begin{solution}
        Il parallelismo dinamico è una funzionalità introdotta dalle CC 3.5 che permette a un kernel in esecuzione di lanciare altri kernel, senza passare dall'host. 
        
        Elimina la necessità di comunicare con la CPU e permette pattern di programmazione ricorsivi e data-dependent. Si possono generare dinamicamente kernel in base ai dati, senza doverli richiedere alla CPU. Il lavoro può essere adattato in base a decisioni data-driven.
        
        Da tenere sotto controllo il numero di kernel lanciati, solitamente non è necessario che ogni thread lanci un nuovo kernel.
        
        Si ha una sincronizzazione implicita tra padre e figlio: il padre non può terminare prima del figlio. Rimane la possibilità di avere sincronizzazione esplicita.
    \end{solution}
    
    \question Spiegare i diversi tipi di stream.
    
    \begin{solution}
        Uno stream CUDA è una sequenza di operazioni CUDA asincrone, eseguite nell'ordine fornito dall'host dalla GPU. Ogni stream è asincrono rispetto all'host ed è indipendente rispetto ad altri stream.
        
        I tipi di stream sono: 
        \begin{itemize}
            \item NULL stream o default stream: si tratta dello stream predefinito, dichiarato implicitamente, usato per i lanci di kernel quando non specificato altrimenti (oppure usando \texttt{0} come parametro). Il suo comportamento varia in base alla flag di compilazione \texttt{--default-stream}
            \begin{itemize}
                \item \texttt{legacy}: gli stream NULL sono bloccanti rispetto agli altri stream, quindi le operazioni sullo stream NULL possono essere seguite solo quando hanno terminato tutti gli altri stream e viceversa 
                
                \item \texttt{per-thread}: ogni thread host ottiene il suo stream di default e si comportano come stream regolari, non sono bloccanti
            \end{itemize}
            
            \item Stream dichiarati esplicitamente o non-NULL: si possono creare stream tramite primitive come \texttt{cudaStreamCreate()} e \texttt{cudaStreamCreateWithFlags()}. Di default sono indipendenti tra loro e bloccanti rispetto al NULL-stream, ma questo comportamento può essere modificato tramite flag
        \end{itemize}
        
        Si possono anche avere stream con priorità, \texttt{cudaStreamCreateWithPriority()}: uno stream con priorità più alta può prelazionare lavoro in esecuzione con priorità più bassa.
    \end{solution}
    
    \question Mostrare l'architettura di uno SM.
    
    \begin{solution}
        Le GPU sono costituite da array di Streaming Multiprocessor SM, ognuno dei quali è pensato per supportare l'esecuzione concorrente di centinaia di thread. Si divide in gruppi di 32 thread chiamati "warp".
        
        Ogni SM al suo interno è composto da: 
        \begin{itemize}
            \item CUDA Core: le ALU per le operazioni intere o floating point
            
            \item Warp scheduler: a ogni ciclo di clock decidono quali warp sono pronti e possono essere mandati in esecuzione
            
            \item Dispatch unit: invia le istruzioni del warp selezionato alle varie execution unit
            
            \item Special Function Unit SFU: usate per calcoli complessi, svolti in modo hardware
            
            \item Eventuali unità specializzate, come Tensor Core o FP64
            
            \item Load/Store Unit LSU: per la gestione delle operazioni di lettura/scrittura in shared memory/cache L1
            
            \item Register file: insieme dei registri per i thread di uno SM, la dimensione limita il numero di thread residenti concorrentemente 
            
            \item Cache L1/shared memory: memoria condivisa tra i thread del blocco, a bassa latenza
            
            \item Cache L2: condivisa tra tutti gli SM, gestisce il traffico verso la memoria globale
            
            \item Instruction Cache: per ridurre la latenza dovuta al fetch di istruzioni
            
            \item Texture \& constant cache: cache separate per accessi read-only in maniera non sequenziale
        \end{itemize}
    \end{solution}
    
    \question Spiegare la warp divergence e come ovviarla nel caso della reduction.
    
    \begin{solution}
        In CUDA, un warp è un insieme di 32 thread che vengono eseguiti sullo stesso Streaming Multiprocessor SM; la warp divergence si ha quando thread all'interno di uno stesso warp prendono path di esecuzione differenti (causa istruzioni di controllo condizionale).
        
        Quando c'è una divergenza, all'interno di un warp, l'hardware deve serializzare i path di esecuzione, eseguendoli uno dopo l'altro, ogni volta disabilitando i thread che non devono entrare in quel ramo di esecuzione. Riduce il parallelismo all'interno del warp, degradando, anche significativamente le prestazioni.
        
        Per la parallel reduction, l'approccio "naive" consiste nell'imitare la somma strided ricorsiva: al passaggio $i$ si sommano elementi a distanza $2^i$; in parallelo, questo attiverebbe 1 thread ogni $2^{i+1}$, causando divergenza crescente (a ogni step si usano la metà dei thread precedenti, divisi sullo stesso numero di warp).
        
        Per risolvere questo problema vogliamo usare thread adiacenti per fare le somme, "disaccoppiando" l'indice del dato dall'indice del thread. Calcoliamo l'indice del dato di cui si deve occupare ogni thread come \texttt{2*stride*tid}, in questo modo thread adiacenti si occupano di tutte le somme, rimuovendo la divergenza (i thread che andrebbero oltre la dimensione dell'array vanno disattivati).
        
        Riorganizzare i pattern di accesso ai dati per "convertire" gli indici in modo che l'utilizzo dei thread sia allineato alla granularità del warp.
    \end{solution}
    
    \question Come si distingue SIMT di CUDA da SIMD? Fare un esempio in cui CUDA si comporta in maniera SIMD.
    
    \begin{solution}
        \textbf{Single Instruction Multiple Data SIMD:} Si tratta di un modello in cui, secondo la tassonomia di Flynn, sono presenti più unità di elaborazione e tutte eseguono lo stesso flusso di istruzioni, ciascuna operando su dati diversi.  
        
        \textbf{Single Instruction Multiple Thread SIMT:} Modello introdotto da CUDA che estende SIMD, fornendo a ogni unità di esecuzione (thread) la possibilità di divergere dalle altre, in base ai dati. 
        
        Il flusso di controllo parte parallelo, ma, in base ai dati, ogni thread può intraprendere un flusso diverso. Per fare ciò è necessario che ogni unità di esecuzione possieda un program counter e register set. In realtà, all'interno di CUDA, il PC è uno per ogni warp (gruppo di 32 thread), i quali eseguono le istruzioni in lock-step e nel caso di divergenza i diversi path vanno eseguiti serialmente.
        
        Oltre al costo "architetturale", si ha un costo in termini di performance quando si incontra una divergenza (i path di esecuzione non sono allineati).
        
        Quando tutti i thread eseguono la stessa istruzione, senza divergenze, il modello SIMT si comporta ugualmente a quello SIMD: si ha un'unica istruzione su dati diversi in parallelo.
        
        Banalmente, qualsiasi codice senza possibilità di divergenze si comporta come SIMD
        \begin{minted}{c}
 __global__ void vectorAdd(const float* A, const float* B, 
float* C, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        C[i] = A[i] + B[i];
    }
}
        \end{minted}
        In questo modo tutti i thread all'interno di un warp eseguono la stessa istruzione.
    \end{solution}
    
    \question Ciclo di vita dei thread e organizzazione gerarchica.
    
    \begin{solution}
    	I thread sono l'unità di esecuzione fondamentale in CUDA, il codice viene scritto in CUDA C per l'esecuzione sequenziale la quale viene estesa a migliaia di thread. Ogni thread, durante la sua vita, può passare per più stati di esecuzione, legati anche al modello di esecuzione SIMT e allo scheduling:
    	\begin{itemize}
    		\item In esecuzione/selezionato: il thread è pronto per essere eseguito e il warp scheduler manda in esecuzione il relativo warp; sta eseguendo istruzioni
    		
    		\item In attesa/blocked: il thread non è parte degli warp "selezionati" dal warp scheduler e non può esserlo in quanto "bloccato" da qualcosa (come l'attesa di un trasferimento di memoria)
    		
    		\item Candidato: il thread è pronto, può essere il prossimo ad essere mandato in esecuzione, ma il warp scheduler non lo ha ancora selezionato
    	\end{itemize}
    	In realtà lo stato riguarda il warp di cui fa parte il thread, non il thread singolo.
    	
    	I thread sono suddivisi (logicamente) su due livelli: 
    	\begin{itemize}
    		\item Block: collezione ordinata di thread, che possono cooperare tra loro, indipendenti l'uno con l'altro, ognuno ha dimensione massima di 1024 thread
    		
    		\item Grid: insieme di tutti i blocchi che eseguono lo stesso kernel
    	\end{itemize}
    	
    	A livello fisico: 
    	\begin{itemize}
    		\item i thread vengono mappati sui CUDA Core
    		
    		\item i blocchi vengono mappati sugli SM, ogni SM può eseguire contemporaneamente uno o più blocchi (in base alle caratteristiche degli stessi)
    		
    		\item la grid viene mappata al device, il quale può eseguire una o più grid concorrentemente, appartenenti a kernel diversi
    		
    		\item i thread fisicamente sono raggruppati in gruppi di 32, detti warp; questi dovrebbero, tra loro, avere modello di esecuzione SIMD, pena un degrado delle prestazioni 
    	\end{itemize}
    \end{solution}
    
    \question Cosa sono i warp in SIMT e qual è la loro utilità.
    
    \begin{solution}
    	Nella architettura SIMT di CUDA, un warp è il raggruppamento minimo di thread all'interno di uno SM che la GPU esegue in lock-step. Un warp è composto da 32 thread e ognuno di questi esegue la stessa istruzione nello stesso ciclo di clock. 
    	
    	All'interno del modello SIMT l'esecuzione parte parallela, ma ogni thread può intraprendere un percorso di esecuzione diverso, raggruppando i thread in gruppi da 32 si può ridurre l'overhead dovuto a controllo e fetching delle istruzioni. Questo può però portare a problemi di divergenza: thread all'interno di un singolo warp devono eseguire la stessa istruzione, se sono presenti path di esecuzione diversi questi vanno eseguiti serialmente, degradando le prestazioni.
    	
    	Inoltre, uno SM decide quali warp mandare in esecuzione tra quelli candidati/pronti, rendendo inattivi gli warp che richiedono attesa, mascherando così la latenza dovuta a operazioni come trasferimenti di memoria. Si vuole massimizzare l'occupancy, ovvero il rapporto tra thread attivi e thread massimi sostenibili, misurato in warp. 
    \end{solution}
    
    \question Esempio e vantaggi di loop unrolling.
    
    \begin{solution}
    	Il loop unrolling è una tecnica usata per ottimizzare i cicli: questi vengono "srotolati" in modo da ridurre l'overhead dovuto alle operazioni di controllo, ridurre il branching e i salti condizionali, aumentando così il livello di parallelismo. Si copia il corpo del loop un numero $n$ di volte, chiamato unroll factor. Lo si può fare manualmente, in alternativa, CUDA fornisce direttive di compilazione.
    	
    	Esempio di loop unrolling: 
    	\begin{minted}{c}
for (int i = 0; i < 4; i++){
	A[base] += B[base + i];
}
    	\end{minted}
    	Può diventare:
    	\begin{minted}{c}
A[base] += B[base + 0];
A[base] += B[base + 1];
A[base] += B[base + 2];
A[base] += B[base + 3];
    	\end{minted}
    	
    	Lo si dovrebbe usare quando il corpo del ciclo è semplice e il numero di iterazioni è multiplo dell'unroll factor. Un unrolling troppo spinto potrebbe causare bloating del codice e riduzione delle performance dovuta a mancanza di spazio in cache/instruction buffer.
    \end{solution}
    
    \question Cos'è la scalabilità? Come vengono misurati i tempi e lo speedup.
    
    \begin{solution}
    	La scalabilità è una proprietà desiderabile per qualsiasi applicazione parallela, fa riferimento alla capacità di eseguire parti di un calcolo in modo concorrente per risolvere velocemente problemi anche di grandi dimensioni; di conseguenza, è la capacità di un'applicazione di trarre vantaggio dall'aumento delle risorse, in maniera efficiente.
    	
    	 Lo speed-up è definito come il rapporto tra il tempo di esecuzione sequenziale $T(1)$ e il tempo di esecuzione su $p$ dispositivi $T(p)$:
    	 $$ S(p) = \frac{T(1)}{T(p)} $$
    	 L'efficienza è lo speeup normalizzato per il numero di risorse.
    	 
    	 I tempi, in ambito CUDA, richiedono di tenere conto dell'esecuzione parallela o asincrona: 
    	 \begin{itemize}
    	 	\item Per tenere traccia solamente dell'esecuzione dei kernel si possono usare gli eventi di timing forniti da CUDA. Tra due eventi, i quali si possono registrare tramite \texttt{cudaEventRecord()}, si può fare \texttt{cudaEventElapsedTime()} per trovare il tempo tra loro (in ms). Questo non tiene conto dell'overhead dovuto ad allocazione e trasferimento della memoria
    	 	
    	 	\item I tempi possono essere misurati dal "punto di vista" dell'host tramite normali primitive di sistema/C, in questo modo si può considerare anche l'overhead dovuto ad allocazione e trasferimento della memoria; da ricordare che il lancio dei kernel è asincrono, quindi prima di prendere la misurazione del tempo finale è necessario sincronizzare il device tramite \texttt{cudaDeviceSynchronize()}
    	 	
    	 	\item Gli strumenti di profiling forniti da Nvidia permettono un'analisi dettagliata delle prestazioni, inclusi i tempi di esecuzione del kernel
    	 \end{itemize}
    \end{solution}
    
    \question Cosa sono le operazioni atomiche e come vengono usate per il calcolo dell'istogramma di immagini RGB.
    
    \begin{solution}
    	Le operazioni atomiche sono operazioni (solamente matematiche) la quale esecuzione non può essere interrotta da altri thread. Sono funzioni, come ad esempio \texttt{atomicAdd()} che vengono tradotte in operazioni singole. Servono a evitare race conditions per l'aggiornamento di dati da parte di thread multipli.
    	
    	Il calcolo dell'istogramma di immagini RGB è una analisi delle frequenze per ogni tono presente in un'immagine. Un'idea per calcolarlo in maniera parallela è:
    	\begin{itemize}
    		\item Allocare una struttura dati per memorizzare i valori delle frequenze (basta un array lungo \texttt{3 * 256})
    		
    		\item Assegnare a ogni thread un pixel
    		
    		\item Ogni thread aumenta di 1 il valore della cella corrispondente al tono di colore del pixel assegnatogli
    	\end{itemize}
    	
    	Quest'ultimo incremento deve essere atomico, altrimenti si potrebbe incorrere in race conditions e di conseguenza valori non consistenti al termine dell'esecuzione.
    \end{solution}
\end{questions}