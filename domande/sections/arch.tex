% !TeX spellcheck = it_IT
\section{Architettura}

\begin{questions}
    \question Parallelismo dinamico.
    
    \begin{solution}
        Il parallelismo dinamico è una funzionalità introdotta dalle CC 3.5 che permette a un kernel in esecuzione di lanciare altri kernel, senza passare dall'host. 
        
        Elimina la necessità di comunicare con la CPU e permette pattern di programmazione ricorsivi e data-dependent. Si possono generare dinamicamente kernel in base ai dati, senza doverli richiedere alla CPU. Il lavoro può essere adattato in base a decisioni data-driven.
        
        Da tenere sotto controllo il numero di kernel lanciati, solitamente non è necessario che ogni thread lanci un nuovo kernel.
        
        Si ha una sincronizzazione implicita tra padre e figlio: il padre non può terminare prima del figlio. Rimane la possibilità di avere sincronizzazione esplicita.
    \end{solution}
    
    \question Spiegare i diversi tipi di stream.
    
    \begin{solution}
        Uno stream CUDA è una sequenza di operazioni CUDA asincrone, eseguite nell'ordine fornito dall'host dalla GPU. Ogni stream è asincrono rispetto all'host ed è indipendente rispetto ad altri stream.
        
        I tipi di stream sono: 
        \begin{itemize}
            \item NULL stream o default stream: si tratta dello stream predefinito, dichiarato implicitamente, usato per i lanci di kernel quando non specificato altrimenti (oppure usando \texttt{0} come parametro). Il suo comportamento varia in base alla flag di compilazione \texttt{--default-stream}
            \begin{itemize}
                \item \texttt{legacy}: gli stream NULL sono bloccanti rispetto agli altri stream, quindi le operazioni sullo stream NULL possono essere seguite solo quando hanno terminato tutti gli altri stream e viceversa 
                
                \item \texttt{per-thread}: ogni thread host ottiene il suo stream di default e si comportano come stream regolari, non sono bloccanti
            \end{itemize}
            
            \item Stream dichiarati esplicitamente o non-NULL: si possono creare stream tramite primitive come \texttt{cudaStreamCreate()} e \texttt{cudaStreamCreateWithFlags()}. Di default sono indipendenti tra loro e bloccanti rispetto al NULL-stream, ma questo comportamento può essere modificato tramite flag
        \end{itemize}
        
        Si possono anche avere stream con priorità, \texttt{cudaStreamCreateWithPriority()}: uno stream con priorità più alta può prelazionare lavoro in esecuzione con priorità più bassa.
    \end{solution}
    
    \question Mostrare l'architettura di uno SM di cui è composta la GPU. Quali sono le unità hardware?
    
    \begin{solution}
        Le GPU sono costituite da array di Streaming Multiprocessor SM, ognuno dei quali è pensato per supportare l'esecuzione concorrente di centinaia di thread. Si divide in gruppi di 32 thread chiamati "warp".
        
        Ogni SM al suo interno è composto da: 
        \begin{itemize}
            \item CUDA Core: le ALU per le operazioni intere o floating point
            
            \item Warp scheduler: a ogni ciclo di clock decidono quali warp sono pronti e possono essere mandati in esecuzione
            
            \item Dispatch unit: invia le istruzioni del warp selezionato alle varie execution unit
            
            \item Special Function Unit SFU: usate per calcoli complessi, svolti in modo hardware
            
            \item Eventuali unità specializzate, come Tensor Core o FP64
            
            \item Load/Store Unit LSU: per la gestione delle operazioni di lettura/scrittura in shared memory/cache L1
            
            \item Register file: insieme dei registri per i thread di uno SM, la dimensione limita il numero di thread residenti concorrentemente 
            
            \item Cache L1/shared memory: memoria condivisa tra i thread del blocco, a bassa latenza
            
            \item Cache L2: condivisa tra tutti gli SM, gestisce il traffico verso la memoria globale
            
            \item Instruction Cache: per ridurre la latenza dovuta al fetch di istruzioni
            
            \item Texture \& constant cache: cache separate per accessi read-only in maniera non sequenziale
            
            \item Barrier \& Synchronization Unit: implementano primitive di sincronizzazione a livello di blocco
            
            \item Interfacce verso L2 e rete di connessione tra SM
        \end{itemize}
    \end{solution}
    
    \question Spiegare la warp divergence e come ovviarla nel caso della reduction.
    
    \begin{solution}
        In CUDA, un warp è un insieme di 32 thread che vengono eseguiti sullo stesso Streaming Multiprocessor SM; la warp divergence si ha quando thread all'interno di uno stesso warp prendono path di esecuzione differenti (causa istruzioni di controllo condizionale).
        
        Quando c'è una divergenza, all'interno di un warp, l'hardware deve serializzare i path di esecuzione, eseguendoli uno dopo l'altro, ogni volta disabilitando i thread che non devono entrare in quel ramo di esecuzione. Riduce il parallelismo all'interno del warp, degradando, anche significativamente, le prestazioni.
        
        Per la parallel reduction, l'approccio "naive" consiste nell'imitare la somma strided ricorsiva: al passaggio $i$ si sommano elementi a distanza $2^i$; in parallelo, questo attiverebbe 1 thread ogni $2^{i+1}$, causando divergenza crescente (a ogni step si usano la metà dei thread precedenti, divisi sullo stesso numero di warp).
        
        Per risolvere questo problema vogliamo usare thread adiacenti per fare le somme, "disaccoppiando" l'indice del dato dall'indice del thread. Calcoliamo l'indice del dato di cui si deve occupare ogni thread come \texttt{2*stride*tid}, in questo modo thread adiacenti si occupano di tutte le somme, rimuovendo la divergenza (i thread che andrebbero oltre la dimensione dell'array vanno disattivati).
        
        Riorganizzare i pattern di accesso ai dati per "convertire" gli indici in modo che l'utilizzo dei thread sia allineato alla granularità del warp.
    \end{solution}
    
    \question Come si distingue SIMT di CUDA da SIMD? Fare un esempio in cui CUDA si comporta in maniera SIMD.
    
    \begin{solution}
        \textbf{Single Instruction Multiple Data SIMD:} Si tratta di un modello in cui, secondo la tassonomia di Flynn, sono presenti più unità di elaborazione e tutte eseguono lo stesso flusso di istruzioni, ciascuna operando su dati diversi.  
        
        \textbf{Single Instruction Multiple Thread SIMT:} Modello introdotto da CUDA che estende SIMD, fornendo a ogni unità di esecuzione (thread) la possibilità di divergere dalle altre, in base ai dati. 
        
        Il flusso di controllo parte parallelo, ma, in base ai dati, ogni thread può intraprendere un flusso diverso. Per fare ciò è necessario che ogni unità di esecuzione possieda un program counter e register set. I thread all'interno di un warp eseguono le istruzioni in lock-step e, nel caso di divergenza, i diversi path vanno eseguiti serialmente.
        
        Oltre al costo "architetturale", si ha un costo in termini di performance quando si incontra una divergenza (i path di esecuzione non sono allineati).
        
        Quando tutti i thread eseguono la stessa istruzione, senza divergenze, il modello SIMT si comporta ugualmente a quello SIMD: si ha un'unica istruzione su dati diversi in parallelo.
        
        Banalmente, qualsiasi codice senza possibilità di divergenze si comporta come SIMD
        \begin{minted}{cuda}
 __global__ void vectorAdd(const float *A, const float *B, 
float *C, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        C[i] = A[i] + B[i];
    }
}
        \end{minted}
        In questo modo tutti i thread all'interno di un warp eseguono la stessa istruzione.
    \end{solution}
    
    \question Ciclo di vita dei thread e organizzazione gerarchica. Come conviene organizzarli per lavorare su matrici?
    
    \begin{solution}
    	I thread sono l'unità di esecuzione fondamentale in CUDA, il codice viene scritto in CUDA C per l'esecuzione sequenziale la quale viene estesa a migliaia di thread. Ogni thread, durante la sua vita, può passare per più stati di esecuzione, legati anche al modello di esecuzione SIMT e allo scheduling:
    	\begin{itemize}
    		\item In esecuzione/selezionato: il thread è pronto per essere eseguito e il warp scheduler manda in esecuzione il relativo warp; sta eseguendo istruzioni
    		
    		\item In attesa/blocked: il thread non è parte degli warp "selezionati" dal warp scheduler e non può esserlo in quanto "bloccato" da qualcosa (come l'attesa di un trasferimento di memoria)
    		
    		\item Candidato: il thread è pronto, può essere il prossimo ad essere mandato in esecuzione, ma il warp scheduler non lo ha ancora selezionato
    	\end{itemize}
    	In realtà lo stato riguarda il warp di cui fa parte il thread, non il thread singolo.
    	
    	I thread sono suddivisi (logicamente) su due livelli: 
    	\begin{itemize}
    		\item Block: collezione ordinata di thread, che possono cooperare tra loro, indipendenti l'uno con l'altro; ogni block ha dimensione massima di 1024 thread
    		
    		\item Grid: insieme di tutti i blocchi che eseguono lo stesso kernel
    	\end{itemize}
    	
    	A livello fisico: 
    	\begin{itemize}
    		\item i thread vengono mappati sui CUDA Core
    		
    		\item i blocchi vengono mappati sugli SM, ogni SM può eseguire contemporaneamente uno o più blocchi (in base alle caratteristiche degli stessi)
    		
    		\item la grid viene mappata al device, il quale può eseguire una o più grid concorrentemente, appartenenti a kernel diversi
    		
    		\item i thread fisicamente sono raggruppati in gruppi di 32, detti warp; questi dovrebbero, tra loro, avere modello di esecuzione SIMD, pena degrado delle prestazioni 
    	\end{itemize}
        
        I thread possono essere organizzati su blocchi e griglie a 1, 2 o 3 dimensioni. Per lavorare su matrici lo schema più intuitivo è quello di usare blocchi e grid bidimensionali in modo da coprire tutto l'input (grid) dividendolo in sotto-matrici (blocchi).
    \end{solution}
    
    \question Cosa sono i warp in SIMT e qual è la loro utilità.
    
    \begin{solution}
    	Nella architettura SIMT di CUDA, un warp è il raggruppamento minimo di thread all'interno di uno SM che la GPU esegue in lock-step. Un warp è composto da 32 thread e ognuno di questi esegue la stessa istruzione nello stesso ciclo di clock. 
    	
    	All'interno del modello SIMT l'esecuzione parte parallela, ma ogni thread può intraprendere un percorso di esecuzione diverso, raggruppando i thread in gruppi da 32 si può ridurre l'overhead dovuto a controllo e fetching delle istruzioni. Questo può però portare a problemi di divergenza: thread all'interno di un singolo warp devono eseguire la stessa istruzione, se sono presenti path di esecuzione diversi questi vanno eseguiti serialmente, degradando le prestazioni.
    	
    	Inoltre, uno SM decide quali warp mandare in esecuzione tra quelli candidati/pronti, rendendo inattivi gli warp che richiedono attesa, mascherando così la latenza dovuta a operazioni come trasferimenti di memoria. Si vuole massimizzare l'occupancy, ovvero il rapporto tra thread attivi e thread massimi sostenibili, misurato in warp. 
    \end{solution}
    
    \question Cos'è la scalabilità? Come vengono misurati i tempi e lo speedup.
    
    \begin{solution}
    	La scalabilità è una proprietà desiderabile per qualsiasi applicazione parallela, fa riferimento alla capacità di eseguire parti di un calcolo in modo concorrente per risolvere velocemente problemi anche di grandi dimensioni; di conseguenza, è la capacità di un'applicazione di trarre vantaggio dall'aumento delle risorse, in maniera efficiente.
    	
    	 Lo speed-up è definito come il rapporto tra il tempo di esecuzione sequenziale $T(1)$ e il tempo di esecuzione su $p$ dispositivi $T(p)$:
    	 $$ S(p) = \frac{T(1)}{T(p)} $$
    	 L'efficienza è lo speeup normalizzato per il numero di risorse.
    	 
    	 I tempi, in ambito CUDA, richiedono di tenere conto dell'esecuzione parallela o asincrona: 
    	 \begin{itemize}
    	 	\item Per tenere traccia solamente dell'esecuzione dei kernel si possono usare gli eventi di timing forniti da CUDA. Tra due eventi, i quali si possono registrare tramite \texttt{cudaEventRecord()}, si può fare \texttt{cudaEventElapsedTime()} per trovare il tempo tra loro (in ms). Questo non tiene conto dell'overhead dovuto ad allocazione e trasferimento della memoria
    	 	
    	 	\item I tempi possono essere misurati dal "punto di vista" dell'host tramite normali primitive di sistema/C, in questo modo si può considerare anche l'overhead dovuto ad allocazione e trasferimento della memoria; da ricordare che il lancio dei kernel è asincrono, quindi prima di prendere la misurazione del tempo finale è necessario sincronizzare il device tramite \texttt{cudaDeviceSynchronize()}
    	 	
    	 	\item Gli strumenti di profiling forniti da Nvidia permettono un'analisi dettagliata delle prestazioni, inclusi i tempi di esecuzione del kernel
    	 \end{itemize}
    \end{solution}
    
    \question Cosa sono le operazioni atomiche e come vengono usate per il calcolo dell'istogramma di immagini RGB.
    
    \begin{solution}
    	Le operazioni atomiche sono operazioni (solamente matematiche) la quale esecuzione non può essere interrotta da altri thread. Sono funzioni, come ad esempio \texttt{atomicAdd()} che vengono tradotte in operazioni singole. Servono a evitare race conditions per l'aggiornamento di dati da parte di thread multipli.
    	
    	Il calcolo dell'istogramma di immagini RGB è una analisi delle frequenze per ogni tono presente in un'immagine. Un'idea per calcolarlo in maniera parallela è:
    	\begin{itemize}
    		\item Allocare una struttura dati per memorizzare i valori delle frequenze (basta un array lungo \texttt{3 * 256})
    		
    		\item Assegnare a ogni thread un pixel
    		
    		\item Ogni thread aumenta di 1 il valore della cella corrispondente al tono di colore del pixel assegnatogli
    	\end{itemize}
    	
    	Quest'ultimo incremento deve essere atomico, altrimenti si potrebbe incorrere in race conditions e di conseguenza valori non consistenti al termine dell'esecuzione.
    \end{solution}
    
    \question Cosa indica la compute capability di una GPU.
    
    \begin{solution}
        Con il termine "compute capability" si intende la versione dell'architettura CUDA supportata da una GPU Nvidia, espressa sotto forma di numero.
        
        Permette di stabilire le feature e funzionalità hardware disponibili. Viene usato in fase di compilazione per stabilire l'architettura per cui compilare.
    \end{solution}
    
    \question Come vengono schedulati i blocchi sugli SM?
    
    \begin{solution}
        Quando viene invocato un kernel, si ha una coda di blocks che aspettano di andare in esecuzione, il block scheduler assegna ogni blocco al primo SM che ha risorse libere a sufficienza. Vengono assegnati dinamicamente agli SM in base alla disponibilità. Un blocco di thread viene assegnato a un solo SM e vi rimane fino al termine della sua esecuzione; molteplici blocchi possono risiedere su un singolo SM.
        
        Ogni SM ha un limite di risorse (in termini di registri, shared memory, numero di thread, \dots) e possiede uno o più warp scheduler che si occupano di decidere, a ogni ciclo di clock, quali warp, tra quelli attivi, mandare in esecuzione; un warp attivo può essere in uno di tre stati: 
        \begin{itemize}
            \item Candidato: pronto per essere mandato in esecuzione
            
            \item Selezionato: in esecuzione
            
            \item Bloccato: in attesa di qualcosa, ad esempio un trasferimento di memoria
        \end{itemize}
        
        Il warp scheduler cambia dinamicamente il warp da mandare in esecuzione in modo da mantenere alta l'occupancy (rapporto tra warp attivi e warp teoricamente sostenibili) e nascondere la latenza dovuta ad alcuni tipi di operazioni.
    \end{solution}
    
    \question Che livello di concorrenza permettono di ottenere i CUDA streams.
    
    \begin{solution}
        I CUDA Streams sono sequenze di operazioni eseguite sulla GPU in maniera asincrona rispetto all'host e (generalmente) indipendenti l'uno dall'altro (stream non-NULL non sono bloccanti a vicenda).
        
        L'esecuzione di un kernel può cominciare alla fine dei trasferimenti di memoria necessari, ma con grandi quantità di dati sarebbe preferibile caricare una porzione dei dati e cominciare l'esecuzione, lasciando al DMA il caricamento dei dati restanti, parallelamente all'esecuzione del kernel. Gli stream permettono di fare questo, abilitano la concorrenza tra trasferimenti di memoria e kernel.
        
        Permettono la sovrapposizione di trasferimento dati ed esecuzione del kernel.
    \end{solution}
    
    \question Come può la divergenza causare deadlock?\footnote{non sono sicuro, penso non causi deadlock perché tutto lock-step, ma non ho capito e sono stanco}
    
    \begin{solution}
        La divergenza accade quando thread all'interno di uno stesso warp seguono flussi di esecuzione separati. Per risolvere il problema esistono barriere di sincronizzazione a livello di warp come \texttt{\_\_syncwarp(mask)}, queste lasciano i thread che la raggiungono in attesa fino a quando tutti i thread di uno stesso warp non arrivano a quel punto nell'esecuzione.
        
        \st{Le barriere di sincronizzazione possono causare problemi quando condizionate: se solo una porzione dei thread all'interno di un warp raggiunge il \texttt{\_\_syncwarp()} dietro un \texttt{if} in cui gli altri non ci arriveranno mai, lasciando gli altri in attesa infinita: deadlock}.
    \end{solution}
    
\end{questions}