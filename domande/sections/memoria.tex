% !TeX spellcheck = it_IT
\section{Memoria}

\begin{questions}
    \question Cosa sono local e constant memory?
    
    \begin{solution}
        In CUDA è presente una gerarchia di memorie, con diversi tipi di memoria al suo interno, ciascuno con dimensioni, banda e scopi specifici. 
        
        Local e constant memory sono due tipi di memoria programmabile esposti al programmatore: 
        \begin{itemize}
            \item \textbf{Local memory}: memoria off-chip (quindi molto lenta), locale ai thread; risiede in global memory. Da CC 2.0 parti di questa sono in cache L1 e L2.
            
            Viene usata per variabili "grandi" (o la cui dimensione non è nota a compile time), oltre che per lo spilling dei registri (quando il kernel usa troppe variabili).
            
            \item \textbf{Constant memory}: si tratta di uno spazio di memoria di sola lettura, accessibile da tutti i thread. La si può dichiarare usando il qualificatore \texttt{\_\_constant\_\_}. Sono 64k per tutte le CC off-chip, con 8k di cache dedicata in ogni SM. Ha scope globale va dichiarata staticamente al di fuori dei kernel.
            
            Viene usata quando tutti i thread devono leggere dalla stessa locazione (raggiunge l'efficienza dei registri); in altri casi le performance sono significativamente minori.
        \end{itemize}
        
        In sintesi: local memory è lenta, serve quando registri e shared memory non bastano, la constant memory è una zona di sola lettura, ideale per accessi broadcast a piccole tabelle condivise.
    \end{solution}
    
    \question Cosa sono le memorie pinned, zero copy e unified.
    
    \begin{solution}
        La memoria \textbf{pinned} è memoria host non paginabile dal sistema operativo, ovvero non può essere fatto lo swap su disco di quella zona di memoria. La si può allocare con \texttt{cudaHostAlloc()}, permette trasferimenti asincroni con maggiore throughput rispetto alla memoria paginabile (evita overhead dovuto al pinning temporaneo). Da notare che l'allocazione eccessiva potrebbe degradare le performance host.
        
        La memoria \textbf{zero copy} si basa su memoria pinned mappata nello spazio di indirizzamento del device, permette alla GPU di accedere direttamente a pagine di memoria host senza copie esplicite di memoria. Può semplificare la programmazione, ma ha latenza più alta della global memory (i dati devono passare su PCIe) e banda limitata.
        
        La memoria \textbf{unified} è un modello di memoria automatico in cui host e device condividono lo spazio di indirizzamento, tutte le CPU e GPU del sistema possono accedere a questa memoria. Il sistema sottostante si occupa di gestire le migrazioni di memoria secondo necessità, in maniera trasparente all'applicazione. Può essere allocata con \texttt{cudaMallocManaged()} e permette di semplificare notevolmente la programmazione, evitando tutte le copie di dati esplicite, ma si ha overhead di migrazione quando viene fatto l'accesso ai dati.
    \end{solution}
    
    \question Come usare la shared memory per le convoluzioni.
    
    \begin{solution}
        Un possibile schema per la convoluzione usando la shared memory è quello del tiling: a ogni blocco della grid viene affidato un "tile" (sotto-blocco) dell'immagine in input, ogni thread nel blocco si occupa di un prodotto della convoluzione. 
        
        In shared memory vanno caricati tutti i possibili dati a cui il blocco deve fare accesso, ovvero la porzione dell'immagine di cui si occupa, assieme all'area data dal raggio della maschera di convoluzione (un "alone" attorno al sotto-blocco stesso).
        
        Rimane il problema di come caricare i blocchi in shared memory: i dati sono più del blocco stesso, sarebbe ottimale dividere il più equamente possibile il carico di lavoro. 
        
        Un'idea può essere dare un ordine ai thread e "ripetere" il thread block sopra la sotto-matrice formata da tutti i dati da caricare. Tiling della zona da caricare in smem.
        
        L'uso della shared memory permette di limitare gli accessi alla memoria globale, incrementando la velocità degli accessi successivi, ma sono da tenere in conto possibili bank conflict (la shared memory è divisa in bank, se due o più thread vogliono accedere alla stessa bank vanno serviti serialmente) e la quantità di memoria shared utilizzata influisce sul numero di blocchi attivi concorrentemente su un singolo SM.
    \end{solution}
    
    \question Modalità di accesso alla device memory e performance.
    
    \begin{solution}
        In CUDA si ha una gerarchia di memorie: 
        \begin{itemize}
            \item Registri: allocati per thread, estremamente veloci (latenza praticamente nulla), ma hanno capienza limitata e un uso eccessivo porta a spilling in memoria locale
            
            \item Shared Memory: memoria condivisa tra i thread di uno SM, latenza bassa, organizzata in bank: accessi senza conflitti di bank garantiscono throughput massimo. Dimensione comunque limitata e l'uso che ne fa ogni blocco determina il numero di blocchi che possono essere in esecuzione su uno SM concorrentemente
            
            \item Global Memory: memoria più grande e a più alta latenza ($400 \sim 800$ cicli), accessibile da tutti i thread e dall'host. Permette una buona latenza, ma solo se gli accessi sono coalescenti (raggruppati in transazioni larghe)
            
            \item Local Memory: risiede fisicamente nella global memory, quindi ha la stessa latenza, viene usata per variabili molto grandi o per lo spilling dei registri
            
            \item Constant \& Texture memory: risiedono nella device memory, ma hanno una cache all'interno di ogni SM, sono usate per accessi uniformi read-only, ovvero quando tutti i thread devono accedere a una stessa zona di memoria (hanno la banda della device memory altrimenti). La texture memory è ottimizzata per dati e operazioni su dati espressi sotto forma di matrici
        \end{itemize}
    \end{solution}
    
    \question Memorie statiche e dinamiche.
    
    \begin{solution}
        Le allocazioni di memoria possono essere statiche o dinamiche. Le allocazioni statiche hanno dimensione nota compile-time e possono risiedere a livello di: 
        \begin{itemize}
            \item Global memory: la si può dichiarare tramite \texttt{\_\_device\_\_} (o \texttt{\_\_constant\_\_} se di sola lettura), ha un accesso lento, scope e lifetime globale
            
            \item Shared memory: la si può dichiarare tramite \texttt{\_\_shared\_\_} all'interno del kernel, latenza bassa, ha scope a livello di blocco e lifetime del kernel
            
            \item Registri: variabili locali ai thread, si tratta della memoria più veloce, ma di dimensione limitata
        \end{itemize}
        
        Le allocazioni dinamiche invece sono usate quando la dimensione dei dati è nota solo in esecuzione. Possono essere:
        \begin{itemize}
            \item Device malloc: tramite la primitiva \texttt{cudaMalloc()}, richiede passaggi di memoria espliciti da host a device (e viceversa, \texttt{cudaMemcpy()}), disponibile fino all'istruzione \texttt{cudaFree()}, risiede nella memoria globale del device
            
            \item Pinned memory: non all'interno del device, ma la primitiva \texttt{cudaHostAlloc()} permette di allocare memoria non paginabile dal sistema operativo per trasferimenti più veloci; questa può essere mappata nello spazio di indirizzamento della GPU (memoria zero copy)
            
            \item Unified Memory: la primitiva \texttt{cudaMallocManaged()} permette di avere una zona di memoria con indirizzamento unico per host e device, lasciando la gestione della migrazione dei dati a CUDA; semplice da utilizzare, ma può portare a maggiori latenze se non usata correttamente
            
            \item Shared memory: la memoria condivisa può essere allocata dinamicamente tramite una dichiarazione all'interno del kernel di una variabile adimensionale:
            \begin{minted}{c}
extern __shared__ float s[];
            \end{minted}
            Va poi passato come terzo argomento tra variabili angolari durante il lancio del kernel la dimensione della memoria condivisa:
            \begin{minted}{c}
kernel<<<grid, block, shared_bytes>>>();
            \end{minted}
        \end{itemize}
    \end{solution}
    
    \question Significato e uso della memoria unificata.
    
    \begin{solution}
        La memoria unificata è un modello di gestione della memoria che permette l'utilizzo di un singolo spazio di indirizzamento (puntatore unico) per accedere a dati sia host che device. 
        
        Ha lo scopo di semplificare la gestione della memoria, rendendo trasparenti al programmatore i trasferimenti, gestiti da CUDA. 
        
        La si può allocare tramite la funzione \texttt{cudaMallocManaged()}, è anche presente un parametro \texttt{flag} per specificare se la memoria è condivisa solo con l'host o anche con tutte le altre GPU.
        
        Semplifica lo sviluppo CUDA, ma può portare a un maggiore overhead dovuto alla migrazione (gestita a livello di pagina) rispetto alla gestione della memoria con trasferimenti espliciti.
    \end{solution}
    
    \question Significato e utilità di pattern di accesso alla memoria globale.
    
    \begin{solution}
        I pattern di accesso alla memoria globale sono le strutture riguardanti "come" viene effettuato l'accesso alla memoria da parte dei thread.
        
        Gli accessi alla memoria del device possono avvenire in transazioni da 32, 64 o 128 byte. Spesso le applicazioni sono limitate dal throughput effettivo della memoria, quindi per rendere efficienti i trasferimenti si vuole minimizzare il numero di transazioni. 
        
        Per migliorare le prestazioni è bene ricordare che le istruzioni vengono eseguite a livello di warp, per un dato indirizzo si esegue una operazione di loading/storing e i 32 thread presentano una singola richiesta di accesso, da servire in una o più transazioni.
        
        Gli accessi possono essere:
        \begin{itemize}
            \item Allineati: quando il primo indirizzo della transazione è un multiplo della granularità della cache usata per servire la transazione (32 byte per L1, 128 per L2)
            
            \item Coalescenti: quando i 32 thread accedono a un blocco contiguo di memoria
        \end{itemize}
        
        Per sfruttare al meglio le transazioni di memoria bisogna rispettare allineamento e coalescenza. Al contrario, effettuare accessi non coalescenti/strided significa rendere necessarie più transazioni per la stessa quantità di dati (fino a 32 diverse).
        
        Può essere importante strutturare i dati in modo da avere accessi coalescenti (array of structures vs structures of array).
        
        Le operazioni di scrittura non usano la cache L1, le \texttt{store} vengono cachate solo in L2, prima di essere inviati alla device memory in 1,2 o 4 segmenti da 32 byte.
    \end{solution}
    
    \question Bank e bank conflict.
    
    \begin{solution}
        La shared memory è una memoria condivisa a livello di blocco, locata all'interno dello SM. Questa è organizzata in "banks" paralleli che permettono l'accesso simultaneo ai dati.
        
        La shared memory è quindi suddivisa in blocchi identici, solitamente da 4 byte/una word, e ciascun blocco può servire un solo accesso per ciclo di clock. 
        
        Se più thread accedono in parallelo a indirizzi mappati a bank diversi, questi accessi possono avvenire in parallelo, altrimenti, se due o più thread vogliono accedere allo stesso bank si ha un bank conflict e gli accessi vengono serializzati. Si vogliono quindi evitare i conflitti per utilizzare la massima banda disponibile.
    \end{solution}
    
    \question Pseudocodice del trasferimento in memoria shared per il prodotto matriciale.
    
    \begin{solution}
    	L'idea dietro l'uso della smem è: vogliamo suddividere in tile le zone di memoria della matrici di cui fare il prodotto (divise in \texttt{nblocks} tile della dimensione del block), in modo da poterle caricare con una suddivisione del lavoro più equa possibile. Per il numero di tile \texttt{nblocks} quindi si ripete il processo di 
    	\begin{itemize}
    		\item caricare i valori delle due matrici in smem
    		
    		\item calcolare la somma parziale data dai valori parziali caricati, per ogni cella della matrice prodotto
    	\end{itemize}
    	
    	Una volta terminato, scrivere i valori in memoria globale.
    	
    	Pseudocodice:
    	\begin{center}
    		\begin{minipage}{.9\textwidth}
    			\begin{tcolorbox}[
    				colback=white,
    				sharp corners,
    				boxrule=.3mm,
    				left=20pt,
    				top=0pt,
    				bottom=0pt,
    				colbacktitle=white,
    				coltitle=black
    				]
    				\LinesNumbered
    				\begin{algorithm}[H]
    					\SetAlgoNoEnd
    					\SetKwSty{texttt}
    					\SetArgSty{relax}
    					\texttt{\_\_shared\_\_ As[WIDTH][WIDTH]}; \\
    					\texttt{\_\_shared\_\_ Bs[WIDTH][WIDTH]}; \\
    					\texttt{nblocks =} numero di block nelle sotto-matrici da caricare; \\
    					\For{$i \in$ \texttt{nblocks}}{
    						\texttt{As[tidy][tidx] = A[tidx\_abs + $i$ * WIDTH][tidy\_abs]}; \\
    						\texttt{Bs[tidy][tidx] = B[tidx\_abs][tidy\_abs + $i$ * WIDTH]}; \\
    						\texttt{\_\_syncthreads()}; \\
    						
    						\For{$j \in$ \texttt{WIDTH}}{
    							\texttt{partial\_sum += As[tidy][j] * Bs[j][tidx]}; \\
    						}
    						\texttt{\_\_syncthreads()};\\
    					}
    					\tcp*{Srivere il risultato in memoria globale}
    				\end{algorithm}
    			\end{tcolorbox}
    		\end{minipage}
    	\end{center}
    \end{solution}
\end{questions}