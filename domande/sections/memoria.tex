% !TeX spellcheck = it_IT
\section{Memoria}

\begin{questions}
    \question Cosa sono local e constant memory?
    
    \begin{solution}
        In CUDA è presente una gerarchia di memorie, con diversi tipi di memoria al suo interno, ciascuno con dimensioni, banda e scopi specifici. 
        
        Local e constant memory sono due tipi di memoria programmabile esposti al programmatore: 
        \begin{itemize}
            \item \textbf{Local memory}: memoria off-chip (quindi molto lenta), locale ai thread; risiede in global memory. Da CC 2.0 parti di questa sono in cache L1 e L2.
            
            Viene usata per variabili "grandi" (o la cui dimensione non è nota a compile time), oltre che per lo spilling dei registri (quando il kernel usa troppe variabili).
            
            \item \textbf{Constant memory}: si tratta di uno spazio di memoria di sola lettura, accessibile da tutti i thread. La si può dichiarare usando il qualificatore \texttt{\_\_constant\_\_}. Sono 64k per tutte le CC off-chip, con 8k di cache dedicata in ogni SM. Ha scope globale va dichiarata staticamente al di fuori dei kernel.
            
            Viene usata quando tutti i thread devono leggere dalla stessa locazione (raggiunge l'efficienza dei registri); in altri casi le performance sono significativamente minori.
        \end{itemize}
        
        In sintesi: local memory è lenta, serve quando registri e shared memory non bastano, la constant memory è una zona di sola lettura, ideale per accessi broadcast a piccole tabelle condivise.
    \end{solution}
    
    \question Cosa sono le memorie pinned, zero copy e unified.
    
    \begin{solution}
        La memoria \textbf{pinned} è memoria host non paginabile dal sistema operativo, ovvero non può essere fatto lo swap su disco di quella zona di memoria. La si può allocare con \texttt{cudaHostAlloc()}, permette trasferimenti asincroni con maggiore throughput rispetto alla memoria paginabile (evita overhead dovuto al pinning temporaneo). Da notare che l'allocazione eccessiva potrebbe degradare le performance host.
        
        La memoria \textbf{zero copy} si basa su memoria pinned mappata nello spazio di indirizzamento del device, permette alla GPU di accedere direttamente a pagine di memoria host senza copie esplicite di memoria. Può semplificare la programmazione, ma ha latenza più alta della global memory (i dati devono passare su PCIe) e banda limitata.
        
        La memoria \textbf{unified} è un modello di memoria automatico in cui host e device condividono lo spazio di indirizzamento, tutte le CPU e GPU del sistema possono accedere a questa memoria. Il sistema sottostante si occupa di gestire le migrazioni di memoria secondo necessità, in maniera trasparente all'applicazione. Può essere allocata con \texttt{cudaMallocManaged()} e permette di semplificare notevolmente la programmazione, evitando tutte le copie di dati esplicite, ma si ha overhead di migrazione quando viene fatto l'accesso ai dati.
    \end{solution}
    
    \question Modalità di accesso alla device memory e performance.
    
    \begin{solution}
        In CUDA si ha una gerarchia di memorie: 
        \begin{itemize}
            \item Registri: allocati per thread, estremamente veloci (latenza praticamente nulla), ma hanno capienza limitata e un uso eccessivo porta a spilling in memoria locale
            
            \item Shared Memory: memoria condivisa tra i thread di uno SM, latenza bassa, organizzata in bank: accessi senza conflitti di bank garantiscono throughput massimo. Dimensione comunque limitata e l'uso che ne fa ogni blocco determina il numero di blocchi che possono essere in esecuzione su uno SM concorrentemente
            
            \item Global Memory: memoria più grande e a più alta latenza ($400 \sim 800$ cicli), accessibile da tutti i thread e dall'host. Permette un buon throughput, ma solo se gli accessi sono coalescenti (raggruppati in transazioni larghe)
            
            \item Local Memory: risiede fisicamente nella global memory, quindi ha la stessa latenza, viene usata per variabili molto grandi o per lo spilling dei registri
            
            \item Constant \& Texture memory: risiedono nella device memory, ma hanno una cache all'interno di ogni SM, sono usate per accessi uniformi read-only, ovvero quando tutti i thread devono accedere a una stessa zona di memoria (hanno la banda della device memory altrimenti). La texture memory è ottimizzata per dati e operazioni su dati espressi sotto forma di matrici
        \end{itemize}
    \end{solution}
    
    \question Memorie statiche e dinamiche.
    
    \begin{solution}
        Le allocazioni di memoria possono essere statiche o dinamiche. Le allocazioni statiche hanno dimensione nota compile-time e possono risiedere a livello di: 
        \begin{itemize}
            \item Global memory: la si può dichiarare tramite \texttt{\_\_device\_\_} (o \texttt{\_\_constant\_\_} se di sola lettura), ha un accesso lento, scope e lifetime globale
            
            \item Shared memory: la si può dichiarare tramite \texttt{\_\_shared\_\_} all'interno del kernel, latenza bassa, ha scope a livello di blocco e lifetime del kernel
            
            \item Registri: variabili locali ai thread, si tratta della memoria più veloce, ma di dimensione limitata
        \end{itemize}
        
        Le allocazioni dinamiche invece sono usate quando la dimensione dei dati è nota solo in esecuzione. Possono essere:
        \begin{itemize}
            \item Device malloc: tramite la primitiva \texttt{cudaMalloc()}, richiede passaggi di memoria espliciti da host a device (e viceversa, \texttt{cudaMemcpy()}), disponibile fino all'istruzione \texttt{cudaFree()}, risiede nella memoria globale del device
            
            \item Pinned memory: non all'interno del device, ma la primitiva \texttt{cudaHostAlloc()} permette di allocare memoria non paginabile dal sistema operativo per trasferimenti più veloci; questa può essere mappata nello spazio di indirizzamento della GPU (memoria zero copy)
            
            \item Unified Memory: la primitiva \texttt{cudaMallocManaged()} permette di avere una zona di memoria con indirizzamento unico per host e device, lasciando la gestione della migrazione dei dati a CUDA; semplice da utilizzare, ma può portare a maggiori latenze se non usata correttamente
            
            \item Shared memory: la memoria condivisa può essere allocata dinamicamente tramite una dichiarazione all'interno del kernel di una variabile adimensionale:
            \begin{minted}{cuda}
extern __shared__ float s[];
            \end{minted}
            Va poi passato come terzo argomento tra variabili angolari durante il lancio del kernel la dimensione della memoria condivisa:
            \begin{minted}{cuda}
kernel<<<grid, block, shared_bytes>>>();
            \end{minted}
        \end{itemize}
    \end{solution}
    
    \question Significato e uso della memoria unificata.
    
    \begin{solution}
        La memoria unificata è un modello di gestione della memoria che permette l'utilizzo di un singolo spazio di indirizzamento (puntatore unico) per accedere a dati sia host che device. 
        
        Ha lo scopo di semplificare la gestione della memoria, rendendo trasparenti al programmatore i trasferimenti, gestiti da CUDA. 
        
        La si può allocare tramite la funzione \texttt{cudaMallocManaged()}, è anche presente un parametro \texttt{flag} per specificare se la memoria è condivisa solo con l'host o anche con tutte le altre GPU.
        
        Semplifica lo sviluppo CUDA, ma può portare a un maggiore overhead dovuto alla migrazione (gestita a livello di pagina) rispetto alla gestione della memoria con trasferimenti espliciti.
    \end{solution}
    
    \question Significato e utilità di pattern di accesso alla memoria globale. Distinzioni tra lettura e scrittura.
    
    \begin{solution}
        La global memory è una memoria off-chip (DRAM, divisa dal chip dell'SM su cui il thread è in esecuzione), quindi ad alta latenza ($400\sim 800$ cicli di clock), con scope e lifetime globale. 
        
        I pattern di accesso alla memoria globale sono le strutture riguardanti "come" viene effettuato l'accesso alla memoria da parte dei thread. La global memory ha, infatti, latenza elevata, ma throughput ampio quando gli accessi sono coalescenti.
        
        Gli accessi alla memoria del device possono avvenire in transazioni da 32, 64 o 128 byte. Spesso le applicazioni sono limitate dal throughput effettivo della memoria, quindi per rendere efficienti i trasferimenti si vuole minimizzare il numero di transazioni. 
        
        Per migliorare le prestazioni è bene ricordare che le istruzioni vengono eseguite a livello di warp, per un dato indirizzo si esegue una operazione di loading/storing e i 32 thread presentano una singola richiesta di accesso, da servire in una o più transazioni.
        
        Gli accessi possono essere:
        \begin{itemize}
            \item Allineati: quando il primo indirizzo della transazione è un multiplo della granularità della memoria usata per servire la transazione (32 byte per L1, 128 per L2)
            
            \item Coalescenti: quando i 32 thread accedono a un blocco contiguo di memoria
        \end{itemize}
        
        Per sfruttare al meglio le transazioni di memoria bisogna rispettare allineamento e coalescenza. Al contrario, effettuare accessi non coalescenti/strided significa rendere necessarie più transazioni per la stessa quantità di dati (fino a 32 diverse).
        
        Può essere importante strutturare i dati in modo da avere accessi coalescenti (array of structures vs structures of array).
        
        Le operazioni di scrittura non usano la cache L1, le \texttt{store} vengono cachate solo in L2, prima di essere inviati alla device memory in 1, 2 o 4 segmenti da 32 byte. Accessi allineati e coalescenti sono ugualmente importanti per lettura e scrittura. Inoltre, per la sola lettura esistono zone di memoria dedicate (constant e texture memory); in generale la lettura possiede una gerarchia di memorie più complessa.
    \end{solution}
    
    \question Bank e bank conflict.
    
    \begin{solution}
        La shared memory è una memoria condivisa a livello di blocco, locata all'interno dello SM. Questa è organizzata in "banks" paralleli che permettono l'accesso simultaneo ai dati.
        
        La shared memory è quindi suddivisa in blocchi identici, solitamente da 4 byte/una word, e ciascun blocco può servire un solo accesso per ciclo di clock. 
        
        Se più thread accedono in parallelo a indirizzi mappati a bank diversi, questi accessi possono avvenire in parallelo, altrimenti, se due o più thread vogliono accedere allo stesso bank si ha un bank conflict e gli accessi vengono serializzati. Si vogliono quindi evitare i conflitti per utilizzare la massima banda disponibile.
    \end{solution}
    
    \question Constant memory e il suo utilizzo.
    
    \begin{solution}
        Si tratta di una memoria che risiede nella device memory, con una cache dedicata per ogni SM. La si può definire tramite l'attributo \texttt{\_\_constant\_\_}, permette di ospitare dati di sola lettura, ideale per accessi uniformi. Ha scope globale, va dichiarata al di fuori dei kernel e staticamente.
        
        Ideale per quando tutti i thread all'interno di un warp devono leggere dallo stesso indirizzo di memoria, raggiungendo efficienza simile a quella dei registri, altrimenti le letture vengono serializzate, degradando significativamente le performance.
        
        Può essere inizializzata dall'host usando
        \begin{minted}{cuda}
cudaError_t cudaMemcpyToSymbol(const void* symbol,
    const void* src, size_t count);
        \end{minted}
        Si può anche leggere lato host tramite
        \begin{minted}{cuda}
cudaError_t cudaMemcpyFromSymbol(const void* dst, 
    const void* symbol, size_t count);
        \end{minted}
    \end{solution}
    
    \question Come viene mappata logicamente e fisicamente la shared Memory.
    
    \begin{solution}
        La shared memory è una memoria programmabile a bassa latenza, con scope di blocco (visibile da tutti thread all'interno di uno stesso blocco) e lifetime pari a quello del kernel. La si può dichiarare tramite il qualificatore \texttt{\_\_shared\_\_}, sia staticamente che dinamicamente (aggiungendo \texttt{extern} e specificando la dimensione in fase di chiamata del kernel). Serve come comunicazione a bassa latenza tra thread dello stesso blocco. La dimensione della shared memory è limitata e il numero di blocchi concorrenti è determinato anche dalla smem disponibile.
        
        Fisicamente, si tratta di una memoria on-chip, quindi posizionato sullo stesso chip dello SM, organizzata linearmente in moduli chiamati bank (tipicamente 32). Gli accessi alla shared memory (sia \texttt{load} che \texttt{store}) vengono emessi per warp e la banda massima si ha quando tutti gli warp accedono a bank diversi (word consecutive). Se due thread all'interno dello stesso warp tentano di accedere allo stesso bank si ha un bank conflict, che richiede la serializzazione degli accessi e causa un degrado delle prestazioni.
    \end{solution}
    
    \question Vantaggi della Unified Memory ed esempio di utilizzo.
    
    \begin{solution}
        La memoria unificata (unified memory), introdotta da CUDA 6.0, semplifica la gestione della memoria creando uno spazio di indirizzamento unico tra CPU e GPU. Tramite un unico puntatore si può accedere sia alla memoria host che device. 
        
        Il trasferimento di dati avviene in maniera trasparente al programmatore, gestito dal sistema CUDA. Tra i vantaggi possiamo notare:
        \begin{itemize}
            \item semplicità di programmazione, si tratta di un modello più semplice, non richiede gestione manuale della memoria e porta a codice più comprensibile
            
            \item coerenza automatica della memoria, CUDA si occupa di mantenere automaticamente la coerenza tra host e device, a volte a scapito delle performance
            
            \item over-subscription della memoria, si può allocare più memoria di quanta sia fisicamente disponibile sulla GPU, le pagine verranno spostate in automatico quando necessario, permettendo di usare data set più grandi
        \end{itemize} 
        
        Esempio di utilizzo:
        \begin{minted}{cuda}
// Dichiarazione e popolamento dei dati
float *A, *B;
cudaMallocManaged(&A, size);
cudaMallocManaged(&B, size);
for (int i = 0; i < N; ++i) {
    A[i] = float(i);
}

// Esecuzione del kernel
kernel<<<grid, block>>>(A, B);
cudaDeviceSynchronize();

// Uso dei risultati da parte della CPU
printf("%f", B[0]); // Esempio banale

// Rilascio delle risorse
cudaFree(A);
cudaFree(B);
        \end{minted}
    \end{solution}
    
    \question Come si usa la SMEM?
    
    \begin{solution}
        La shared memory è una memoria a bassa latenza con lifetime del kernel e scope a livello di blocco. La si può dichiarare tramite il qualificatore \texttt{\_\_shared\_\_}:
        \begin{minted}{cuda}
__shared__ float s[size];
        \end{minted}
        
        Se la dimensione della memoria condivisa non è nota a compile time ma deve essere dinamica, si usa la keyword \texttt{extern} e la dimensione va indicata come terzo parametro tra parentesi angolari quando viene chiamato il kernel. Dichiarazione:
        \begin{minted}{cuda}
extern __shared__ float s[];
        \end{minted}
        Dimensione:
        \begin{minted}{cuda}
kernel<<<grid, block, sharedMemorySize>>>();
        \end{minted}
    \end{solution}
    
    
\end{questions}