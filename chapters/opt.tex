% !TeX spellcheck = it_IT
\section{Ottimizzazione delle Prestazioni}

\subsection{Risorse Hardware}

\paragraph{Device Query:} Per indagare le feature disponibili sul device, scoprire le proprietà. Ad esempio: quanti SM sono disponibili, quanta memoria, \dots\\

Per farlo ci sono \href{http://docs.nvidia.com/cuda/cuda-runtime-api}{\texttt{Funzioni delle API runtime di CUDA}} e la CLI utility \href{https://developer.nvidia.com/nvidia-system-management-interface}{\texttt{nvidia-smi}}. Quest'ultimo permette di gestire e monitorare le GPU presenti.\\

Le funzioni: 
\begin{center}
	\texttt{cudaError\_t cudaGetDeviceCount(\&dev\_count)} \\
	\texttt{cudaError\_t cudaGetDeviceProperties(cudaDeviceProp* prop, int device);}
\end{center}
Indaga il numero di device disponibili sul sistema e restituisce le proprietà del device nella struttura \texttt{cudaDeviceProp} (rispettivamente).\\

%TODO
\subsection{Gestione ottimizzata delle risorse}

L'ottimizzazione delle performance si basa su 4 strategie principali:
\begin{itemize}
	\item massimizzare l'utilizzazione tramite massimo parallelismo
	\item ottimizzare l'utilizzo di memoria per avere il throughput di memoria massimo
	\item ottimizzare l'uso di istruzioni per avere il massimo throughput
	\item minimizzare il memory thrashing
\end{itemize}

Che strategie permettono di ottenere le migliori performance per una determinata applicazione dipende da qual'è il fattore limitante all'interno dell'applicazione stessa. Gli sforzi per l'ottimizzazione vanno quindi costantemente direzionati monitorando i fattori che limitano le performance, tramite strumenti come il CUDA profiler. \\

\newpage

\paragraph{Register spilling:} Il massimo numero di registri per thread può essere definito manualmente compile time con l'opzione \texttt{-maxrregcount} e si può indagare (sempre compile time) con \texttt{--ptxas-options=-v}. \\
Limitare il numero porta a fare spilling (quindi usare la memoria locale), ma aumentando il numero di blocchi concorrenti. \\ %(maybe)

\subsection{Profiling}

Nvidia mette a disposizione dei \textbf{developer tools} per effettuare profiling e monitorare le applicazioni. \\

\paragraph{Nsight Compute:} Profiler di livello kernel che fornisce informazioni dettagliate sulle metriche di esecuzione dei kernel CUDA. Permette una misurazione dettagliata delle prestazioni dei kernel (latency, throughput, utilizzo delle risorse, ecc.), analisi delle performance a livello di istruzione e accesso alla memoria, supporto per personalizzare la raccolta di metriche e approfondire l’ottimizzazione delle singole funzioni CUDA. \texttt{ncu, ncu-ui}, CLI e GUI.\\

\paragraph{Nsight Systems:} Offre un'analisi a livello di sistema, ideale per identificare bottleneck nell'interazione tra CPU e GPU. Fornisce una visione d’insieme dell'intero flusso applicativo, monitorando la sincronizzazione tra processi e thread, il trasferimento dei dati e l'esecuzione complessiva. Permette di analizzare come le attività CUDA si integrino con il resto dell'applicazione, evidenziando le possibili ottimizzazioni per bilanciare meglio l’utilizzo di tutte le risorse hardware. \texttt{nsys, nsys-ui}, CLI e GUI.\\

\newpage

%back to main slides
\subsection{Loop Unrolling}
Il loop unrolling può essere utile per ottimizzare i cicli: questi vengono espansi ("srotolati") in modo da ridurre l'effettivo numero di iterazioni necessarie durante l'esecuzione del kernel. Il corpo del ciclo viene riscritto più volte. \\

Questo ha diversi vantaggi, tra cui: 
\begin{itemize}
	\item riduzione dell'overhead dovuto ai controlli del ciclo
	\item eliminazione di salti e riduzione della logica di controllo 
	\item aumento del livello di parallelismo
\end{itemize}

Il numero di copie del corpo del loop create si chiama \textbf{unrolling factor} (quanto è stato "srotolato" il ciclo). Questa tecnica è efficace quando il numero di iterazioni è noto a priori.\\

\paragraph{Warp unrolling:} L'ottimizzazione si può anche migliorare sfruttando il concetto di warp. Tutti i 32 thread all'interno di un solo warp eseguono lo stesso codice in maniera sincrona, si usa questa caratteristica per unrollare il codice di un ciclo in maniera esplicita, eliminando controlli ed eventuali divergenze tra thread. Dato che tutti gli warp eseguono lo stesso codice, l'unrolling garantisce che il flusso di esecuzione rimanga uniforme, riducendo la divergenza (percorsi di codice differenti all'interno del medesimo warp). \\

% End L6

\subsection{Parallelismo dinamico}

Ci siamo mai chiesti se si può lanciare un kernel all'interno di un kernel? Not really, ma potrebbe essere utile (come ad esempio per la ricorsione). Nuova feature introdotta dalle CC 3.5: ogni kernel può lanciare un altro kernel e gestire dipendenze inter-kernel. Elimina la necessità di comunicare con la CPU, rende più semplice creare e ottimizzare pattern di esecuzione ricorsivi e data-dependent. Senza parallelismo dinamico la CPU deve lanciare ogni kernel.\\

L'idea dietro il parallelismo dinamico è generare dinamicamente kernel in base ai dati: se ci sono elementi diversi/zone della matrice di lavoro che richiedono sforzi diversi possiamo fare in modo che i kernel sian \textit{ad hoc} per migliorare l'efficienza.\\

%Esempio: Mandelbrot %Ci saranno le slide, forse, da qualche parte

Senza permettere al kernel di lanciare altri kernel il modello di esecuzione è inefficiente: la CPU non può essere conscia dei dati, ma è lei che deve lanciare \textit{tutti} i kernel, la GPU valuta se è necessario lanciare nuovi kernel (in base ai dati) e tali informazioni vanno passate nuovamente alla CPU per lanciare nuovi kernel.\\

La soluzione è il \textbf{parallelismo dinamico}: la GPU può lanciare nuovi kernel, permette di ridurre la dipendenza dalla CPU e migliorare il throughput del kernel (se fatto bene). Consente carichi di lavoro dinamici senza penalizzare le prestazioni.\\

Vogliamo mettere carico di lavoro dove serve, scegliere la granularità del lavoro in base ai dati. Possiamo posporre la decisione delle dimensioni di blocchi e griglia fino a runtime. Possiamo adattare il lavoro in base a \textbf{decisioni data-driven}, non da schemi fissi come visto fino ad ora.\\

Esempio: un kernel figlio viene chiamato all'interno di un kernel padre e quest'ultimo può utilizzare i risultati prodotti dal figli senza nessuna interazione da parte della CPU
\begin{minted}{c}
__global__ ChildKernel(void* data) {
	//Operate on data
}
__global__ ParentKernel(void* data) {
	ChildKernel<<<16, 1>>>(data);
}
// In Host Code
ParentKernel<<<256, 64>>(data);
\end{minted}

Sarebbe da limitare un attimo l'annidamento: se ogni thread facesse una chiamata a kernel figlio \textit{potrebbero} diventare tanti kernel lanciati; sarebbe carino inserire \textbf{control flow attorno ai lanci}, per esempio limitando il lancio ad 1 per blocco del padre (\texttt{threadIdx.x == 0}).\\

\paragraph{Sincronizzazione:} Si ha una sincronizzazione implicita, il padre non può terminare prima dei figlio, un kernel non è considerato completato finché ha figli attivi. Rimane la possibilità di avere sincronizzazione esplicita, altrimenti il kernel padre non ha garanzie di poter vedere i dati elaborati dal figlio. \\

%Slide 7

\subsection{Librerie CUDA}
Le librerie sono comode e quelle CUDA sono accelerate dalla GPU. Le API di molte di queste sono volutamente simili a quelle della libreria standard. Permettono porting di codice da sequenziale a parallelo con \textit{minimo sforzo}, nessun tempo di mantenimento della libreria.\\

Esempi di librerie CUDA:
\begin{center}
	\begin{tabular}{@{} ll @{}}
		\hline
		\textbf{Libreria}                     & \textbf{Dominio}                                    \\ \hline
		cuFFT (NVIDIA)                        & Fast Fourier Transforms Linear                      \\
		cuBLAS (NVIDIA)                       & Linear Algebra (BLAS Library)                       \\
		cuSPARSE (NVIDIA)                     & Sparse Linear Algebra                               \\
		cuRAND (NVIDIA)                       & Random Number Generation                            \\
		NPP (NVIDIA)                          & Image and Signal Processing                         \\
		CUSP (NVIDIA)                         & Sparse Linear Algebra and Graph Computations        \\
		CUDA Math Library (NVIDIA)            & Mathematics                                         \\
		Trust (terze parti)                   & Parallel Algorithms and Data Structures             \\
		MAGMA (terze parti)                   & Next generation Linear Algebra                      \\ \hline
	\end{tabular}
\end{center}

\paragraph{Workflow tipico:} Per l'utilizzo di una libreria CUDA, il workflow generico è:
\begin{enumerate}
	\item Creare un \textbf{handle} specifico della libreria (per la gestione delle informazioni e relativo contesto in cui essa opera, es. uso degli stream)
	\item \textbf{Allocare la device memory} per gli input e output alle funzioni della libreria (convertirli al formato specifico di uso della liberia, es. converti array 2D in column-major order)
	\item \textbf{Popolare con i dati} nel formato specifico
	\item \textbf{Configurare} le computazioni per l'esecuzione (es. dimensione dei dati)
	\item Eseguire la \textbf{chiamata della funzione} di libreria che avvia la computazione sulla GPU
	\item \textbf{Recuperare i risultati} dalla device memory
	\item Se necessario, \textbf{(ri)convertire i dati} nel formato specifico o nativo dell'applicazione
	\item \textbf{Rilasciare le risorse} CUDA allocate per la data libreria
\end{enumerate}

\subsubsection{cuBLAS - Basic Linear Algebra Subproblems}

Usata per calcolo scientifico ed ingegneristico per problemi di algebra lineare numerica
\begin{itemize}
	\item risoluzione di sistemi lineari
	\item ricerca di autovalori e/o autovettori
	\item calcolo della SVD (valori e vettori singolari)
	\item fattorizzazione di matrici
\end{itemize}

Come BLAS, le funzioni di cuBLAS sono divisi in livelli: 
\begin{itemize}
	\item Livello 1: per operazioni vettore-vettore
	\item Livello 2: per operazioni vettore-matrice
	\item Livello 3: per operazioni matrice-vettore
\end{itemize}

Usa \textbf{column-major order} (leggo le colonne dall'alto verso il basso) perché chiunque ha scritto la libreria è stronzo (colpa di Fortran). Esempio: 
$$
\left[
\begin{array}{c c c}
	1 & 2 & 3 \\
	4 & 5 & 6 \\
	7 & 8 & 9
\end{array}
\right]
\rightarrow \left[\begin{array}{c c c c c c c c c}
	1 & 4 & 7 & 2 & 5 & 8 & 3 & 6  & 9
\end{array}\right]
\quad I(r,c) = c \cdot M  + r 
$$
Dove $(r,c)$ sono le coordinate del valore cercato e $M$ è l'altezza della matrice (dimensioni $M \times N$).\\

\paragraph{Operare con cuBLAS:} L'iter tipico per usare cuBLAS è 
\begin{enumerate}
	\item creare un handle con \texttt{cublasCreateHandle}
	\item allocare la memoria sul device con \texttt{cudaMalloc}
	\item poplare la device memory con gli input necessari usando \texttt{cublasSetVector} e \texttt{cublasSetMatrix}
	\item effettuare le chiamate di libreria necessarie
	\item recuperare i risultati dalla device memory usando \texttt{cublasGetVector} e \texttt{cublasGetMatrix}
	\item rilasciare le risorse CUDA e cuBLAS con \texttt{cudaFree} e \texttt{cublasDestroy}, rispettivamente
\end{enumerate}

\paragraph{Funzioni all'interno di cuBLAS:} Per trasferire vettori da CPU a GPU:
\begin{itemize}
	\item Copia \texttt{n} elementi di dimensione \texttt{elemSize} da \texttt{cpumem} sulla CPU ad un vettore \texttt{gpumem} sulla GPU
	\begin{minted}{c}
cublasSetVector(int n, int elemSize, const void *cpumem, 
	int incx, void *gpumem, int incy)
	\end{minted}
	\item L'inverso di prima (da GPU a CPU)
	\begin{minted}{c}
cublasGetVector(int n, int elemSize, const void *gpumem, 
	int incx, void *cpumem, int incy)
	\end{minted}
\end{itemize}

Per trasferire matrici (sempre column-major order): 
\begin{itemize}
	\item copia una matrice \texttt{rows} $\times$ \texttt{cols}, di elementi grossi \texttt{elemSize}, da \texttt{A} nella memoria CPU a \texttt{B} nella memoria GPU
	\begin{minted}{c}
cublasSetMatrix(int rows, int cols, int elemSize, 
	const void *A, int lda, void *B, int ldb)
	\end{minted}
	esiste anche il corrispettivo \texttt{cublasGetMatrix()} che fa l'inverso
	\item come \texttt{cublasGetMatrix()}, ma asincrono (rispetto all'host), usando il parametro \texttt{stream} fornito
	\begin{minted}{c}
cublasGetMatrixAsync(int rows, int cols, int elemSize, 
	const void *A, int lda, void *B, 
	int ldb, cudaStream_t stream)
	\end{minted}
\end{itemize}

Per gestire la libreria serve un \textbf{handle}, il quale si può generare tramite
\begin{minted}{c}
cublasCreate(cublasHandle_t* handle)
\end{minted}
Viene passato ad ogni chiamata di funzione della libreria successiva. Al termine
\begin{minted}{c}
cublasDestroy(cublasHandle_t* handle)
\end{minted}
per distruggerlo. Il tipo dell'handle è \texttt{cublasHandle\_t}. Esiste un tipo \texttt{cublasStatus\_t} usato per il report degli errori.\\

Per trasferimenti device-device: copia \texttt{n} elementi da \texttt{x} a \texttt{y}:
\begin{minted}{c}
cublasScopy(handle, n, x, incx, y, incy)
\end{minted}

In generale la libreria segue una naming convention \texttt{cublas<T>operation}, dove \texttt{<T>} può essere: 
\begin{itemize}
	\item \texttt{S} per parametri di tipo \texttt{float}
	\item \texttt{D} per \texttt{double}
	\item \texttt{C} per \texttt{complex  floats}
	\item \texttt{Z} per \texttt{complex double}
\end{itemize}
Ad esempio, per l'operazione \texttt{axpy} le funzioni disponibili sono \texttt{cublasSaxpy}, \texttt{cublasDaxpy}, \texttt{cublasCaxpy}, \texttt{cublasZaxpy}.\\

Si usa un valore di tipo \texttt{cublasOperation\_t} per indicare operazioni su matrici all'interno di funzioni: 
\begin{itemize}
	\item \texttt{CUBLAS\_OP\_N} per non-transpose
	\item \texttt{CUBLAS\_OP\_T} per transpose
	\item \texttt{CUBLAS\_OP\_C} per conjugate transpose
\end{itemize}

Per fare
$$ result = \sum_{i=1}^{n} x[k] \cdot y[j], \quad k = 1 + (i-1) \cdot incx, \quad j = 1 + (i-1) \cdot incy $$
tra vettori \texttt{x} e \texttt{y} di \texttt{n} elementi (dimensione dei tali nella naming convention) e mettere il risultato in \texttt{result}
\begin{minted}{c}
cublasStatus_t cublasSdot(cublasHandle_t handle, int n, 
	const float *x, int incx, const float *y, 
	int incy, float result)
\end{minted}

Per fare 
$$ y[i] = \alpha \cdot x[i] + y[i] \quad \forall i \in n $$
con vettori \texttt{x} e \texttt{y} di dimensione \texttt{n}, risultato nel secondo vettore \texttt{y}
\begin{minted}{c}
cublasStatus_t cublasSaxpy(cublasHandle_t handle, int n,
	const float *alpha, const float *x, int incx, 
	const float *y, int incy)
\end{minted}

Per fare 
$$ y = \alpha Ax + \beta y$$
dove $\alpha$ e $\beta$ sono scalari, $A$ è una matrice, $x$ e $y$ sono vettori
\begin{minted}{c}
cublasStatus_t cublasSgemv(cublasHandle_t handle, 
	cublasOperation_t trans, int m, int n, 
	const float *alpha, const float *A, 
	int lda, const float *x, int incx, 
	const float *beta, float *y, int incy)
\end{minted}

Per fare
$$ C = \alpha AB + \beta C$$
dove $\alpha$ e $\beta$ scalari, $A$, $B$ e $C$ matrici
\begin{minted}{c}
cublasStatus_t cublasSgemm(cublasHandle_t handle,
	cublasOperation_t transa, cublasOperation_t transb,
	int m, int n, int k, const float *alpha,
	const float *A, int lda,
	const float *B, int ldb,
	const float *beta, float *C, int ldc)
\end{minted}

\newpage

\subsubsection{cuRAND}

La libreria cuRAND fornisce semplici ed efficienti \textbf{generatori di numeri}. Permette sequenze: 
\begin{itemize}
	\item Pseudo-random: soddisfa proprietà statistiche di una vera sequenza random, ma generata da un algoritmo deterministico
	\item Quasi-random: sequenza di punti $n$-dimensionali uniformemente generati secondo un algoritmo deterministico
\end{itemize}

La libreria si compone di due parti:
\begin{itemize}
	\item \texttt{curand.h} per l'host
	\item \texttt{curand\_kernel.h} per il device
\end{itemize}

\paragraph{Host API:} Dalla \href{https://docs.nvidia.com/cuda/curand/index.html}{\texttt{documentazione}}, passaggi:
\begin{enumerate}
	\item Crea un \textbf{nuovo generatore} del tipo desiderato con \texttt{curandCreateGenerator()}
	\item Setta i \textbf{parametri} del generatore; ad esempio: \texttt{curandSetPseudoRandomGeneratorSeed()} per settare il seed
	\item Alloca la memoria device con \texttt{cudaMalloc()}
	\item Genera i valori casuali necessari con \texttt{curandGenerate()} (o altre funzioni)
	\item Usa i valori
	\item Quando non serve più il generatore va distrutto con \texttt{curandDestroyGenerator()}
\end{enumerate}

Alcune funzioni per l'host: 
\begin{itemize}
	\item Per creare il generatore
	\begin{minted}{c}
curandCreateGenerator(&g, GEN_TYPE)
	\end{minted}
	Dove il parametro \texttt{GEN\_TYPE} può essere \texttt{CURAND\_RNG\_PSEUDO\_DEFAULT}, oppure \texttt{CURAND\_RNG\_PSEUDO\_XORWOW} (differenze trascurabili)
	
	\item Per impostare il seed
	\begin{minted}{c}
curandSetRandomGeneratorSeed(g, SEED)
	\end{minted}
	ma importa poco, uno qualunque va bene (e.g, \texttt{time(NULL)})
	
	\item Per generare una distribuzione
	\begin{minted}{c}
curandGenerate______(…)
	\end{minted}
	dipende dalla distribuzione che si vuole generare, ad esempio: \texttt{curandGenerateUniform(g, src, n)} oppure \texttt{curandGenerateNormal(g, src, n, mean, stddev)}. 
	\item Per distruggere il generatore
	\begin{minted}{c}
curandDestroyGenerator(g)
	\end{minted}
\end{itemize}

La funzione \texttt{curandGenerate()} permette di generare valori in maniera asincrona, molto più veloce per quantità elevate di valori. Usare questa libreria richiederebbe poi di dover passare i dati generati alla GPU (\texttt{src} all'interno della funzione è un puntatore host), introducendo overhead. Per risolvere si può usare la Device API.\\

\paragraph{Device API:} Per generare valori sul device: 
\begin{enumerate}
	\item Pre-allocare un set di cuRAND state objects nella device memory per ogni thread (gestiscono lo stato)
	\item Opzionale, pre-allocare device memory per tenere i valori generati (se devono poi essere passati all'host o essere mantenuti per kernel successivi)
	\item Inizializzare lo stato di tutti gli state objects con una kernal call
	\item Chiamare una funzione cuRAND per generare valori casuali usando gli state objects allocati
	\item Opzionale, trasferire i valori all'host (se è stata allocata la memoria in precedenza)
\end{enumerate}

\newpage

\subsubsection{cuFFT - Fast Fourier Transform}

La libreria cuFFT fornisce una implementazione della \href{https://en.wikipedia.org/wiki/Fast_Fourier_transform}{\texttt{Fast Fourier Transform FFT}} (segnali dal dominio del tempo a frequenze e relativa inversa). L'input della FFT è un sampling di un segnale. La librerie permette trasformate 1D, 2D o 3D. Si basa sugli algoritmi per FFT di \href{https://en.wikipedia.org/wiki/Cooley%E2%80%93Tukey_FFT_algorithm}{\texttt{Cooley-Tukey}} e \href{https://en.wikipedia.org/wiki/Chirp_Z-transform#Bluestein.27s_algorithm}{\texttt{Bluestein}}. Permette una esecuzione asincrona con valori che possono essere \texttt{rea\texttt{l}, \texttt{complex}, \texttt{float}, }double. \\

%s63