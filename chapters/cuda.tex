% !TeX spellcheck = it_IT
\section{Modello CUDA}

\subsection{Thread in CUDA}

Pensare in parallelo significa avere chiaro quali feature la GPU espone al programmatore
\begin{itemize}
	\item Conoscere l'architettura della GPU per scalare su migliaia di thread come fosse uno
	
	\item Gestione basso livello cache permette di sfruttare principio di località
	
	\item Conoscere lo scheduling di blocchi di thread e la gerarchia di thread e di memoria (ridurre latenze)
	
	\item Fare impiego diretto della shared memory (riduce latenze come le cache)
	
	\item Gestire direttamente le sincronizzazioni (barriere tra thread)
\end{itemize}

Si scrive codice in CUDA C (estensione di C) per l'esecuzione sequenziale e lo si estende a migliaia di thread (permette di pensare "ancora" in sequenziale ma eseguire codice in parallelo).

L'host ha una serie di processi in esecuzione e controlla l'ambiente, compreso il lancio delle funzioni kernel sul device. Con "kernel" si intende un programma sequenziale eseguito dalla GPU.

Ogni kernel è asincrono: la CPU lancia il kernel e passa oltre, almeno finché non è necessaria sincronizzazione, come ad esempio per i trasferimenti tra memorie.

Il compilatore \texttt{nvcc} genera codice eseguibile per host e device (fat-binary).

Esempio di \textbf{processing flow}: 
\begin{itemize}
	\item Copiare dati da CPU a GPU, tutto parte dalla CPU
	
	\item Caricare il programma GPU, con tutto il setup necessario, svolto da parte della GPU
	
	\item Al termine della computazione i risultati vengono copiati da GPU a CPU
\end{itemize}

\newpage

La "ricetta" base per cucinare in CUDA:
\begin{enumerate}
	\item Setup dei dati su host (CPU-accessible memory)
	
	\item Alloca memoria per i dati sulla GPU
	
	\item Copia i dati da host a GPU
	
	\item Alloca memoria per output su host
	
	\item Alloca memoria per output su GPU
	
	\item Lancia il kernel su GPU
	
	\item Copia output da GPU a host
	
	\item Libera le memorie
\end{enumerate}

\subsubsection{Organizzazione dei thread}

CUDA presenta una \textbf{gerarchia astratta di thread} strutturata su \textbf{due livelli} che si decompone in 
\begin{itemize}
	\item \textbf{grid}: una griglia ordinata di blocchi

	\item \textbf{block}: una collezione ordinata di thread
\end{itemize}

Grid e block possono avere 1, 2 o 3 dimensioni. Sono possibili 9 combinazioni, ma solitamente si usa la stessa per grid e block. La scelta delle dimensioni è da definire a seconda della struttura dei dati in uso.
\begin{center}
	\includegraphics[width=0.98\linewidth]{img/cuda/grdiblock}
\end{center}

Tutti i blocchi devono essere uguali, in struttura e numero di thread. La griglia replica blocchi tutti uguali.

In qualsiasi caso, in \textbf{ogni blocco} ci possono essere \textbf{al più 1024 thread}; esempi di dimensioni: $(1024, 1, 1)$  o $(32, 16, 2)$. Il totale non può superare 1024.

\paragraph{Thread block:} Un blocco di thread è un gruppo di thread che possono cooperare tra loro mediante:
\begin{itemize}
	\item \textbf{Block-local synchronization}
	
	\item \textbf{Block-local shared memory}
\end{itemize}

La memoria più veloce è condivisa solo dallo stesso blocco, quindi da CUDA 9.0 e CC 3.0+ thread di differenti blocchi possono cooperare come Cooperative Groups.

Tutti i thread in una grid condividono lo stesso spazio di global memory. Una grid rappresenta un processo, ogni processo lanciato dall'host ha una sua grid associata (ogni kernel).

I thread vengono identificati univocamente dalle coordinate: 
\begin{itemize}
	\item \texttt{blockId} (indice del blocco nella grid)
	
	\item \texttt{threadId} (indice di thread nel blocco)
\end{itemize}

Sono variabili built-in, ognuna delle quali con 3 campi: \texttt{x,y,z} (una per ogni possibile dimensione).

\paragraph{Dimensioni di blocchi e thread:} le dimensioni di grid e block sono specificate dalle variabili built-in: 
\begin{itemize}
	\item \texttt{blockDim} (dimensione di blocco, misurata in thread)
	
	\item \texttt{gridDim} (dimensione della griglia, misurata in blocchi)
\end{itemize}

Sono di tipo \texttt{dim3}, un vettore di interi basato su \texttt{uint3}. I campi sono sempre \texttt{x,y,z}. Ogni componente non specificata è inizializzata a 1.

\paragraph{Linearizzare gli indici:} Ovviamente gli indici in blocchi a più dimensioni si possono linearizzare: con due indici $x,y$ posso unificarli facendo $x + y \cdot D_x$, dove $D_x$ è la dimensione della riga.

Possiamo tradurlo in un indice unico per i thread: per griglie e blocchi a 1D ciascuno: 
\begin{center}
	\texttt{IDth = blockIdx.x * blockDim.x + threadIdx.x}
\end{center}
Si può ovviamente scalare a più dimensioni, per ottenere un'indicizzamento unico tra tutte le dimensioni. 

\paragraph{Lanciare un kernel:} Per lanciare un kernel CUDA si aggiungono tra triple parentesi angolari le dimensioni di grid e block.
\begin{minted}{c}
kernel_name <<<grid, block>>>(argument list);
\end{minted}

\paragraph{Runtime API:} Alcune funzioni:
\begin{itemize}
	\item \texttt{cudaDeviceReset()} distrugge tutte le risorse associate al device per il processo corrente, non molto usato ma si può fare
	
	\item \texttt{cudaDeviceSynchronize()} aspetta che la GPU termini l'esecuzione di tutti i task lanciati fino a quel punto, sincronizzazione host device
\end{itemize}

Per effettuare debugging, la \texttt{Synchronize} permette di "scaricare" tutti i \texttt{printf} quando servono. Altrimenti, dato che le chiamate sono asincrone, si rischia che l'applicazione lato CPU termini prima che i \texttt{printf} abbiano avuto modo di essere mostrati.

Un altro mezzo di debugging è \texttt{Kernel<<<1,1>>>}: forza l'esecuzione su un solo blocco e thread, emulando comportamento sequenziale sul singolo dato.

Proprietà dei kernel: 
\begin{center}
	\begin{tabular}{| l | l | p{4cm} |}
		\hline
		\textbf{Qualificatori} & \textbf{Esecuzione} & \textbf{Chiamata} \\
		\hline
		\texttt{\_\_global\_\_} & Eseguito dal device & Dall’host e dalla compute cap. 3 anche dal device \\
		\hline
		\texttt{\_\_device\_\_} & Eseguito dal device & Solo dal device \\
		\hline
		\texttt{\_\_host\_\_} & Eseguito dall’host & Solo dall’host \\
		\hline
	\end{tabular}
\end{center}

\paragraph{Restrizioni del kernel:}
\begin{itemize}
	\item Accede alla sola memoria device
	
	\item Deve restituire un tipo \texttt{void}
	
	\item Non supporta il numero variabile di argomenti
	
	\item Non supporta variabili statiche
	
	\item Non supporta puntatori a funzioni
	
	\item Esibisce un comportamento asincrono rispetto all'host
\end{itemize}

\paragraph{Gestione degli errori:} Si ha un \texttt{enum cudaError\_t} come valore di ritorno di ogni chiamata cuda. Può essere \texttt{success} o \texttt{cudaErrorMemoryAllocation}. Si può usare {cudaError\_t cudaGetLastError(void)} per ottenere il codice dell'ultimo errore.

\paragraph{Misurare tempo con la CPU:} Per misurare il tempo di esecuzione con la CPU serve aspettare il termine dell'esecuzione del kernel lanciato, quindi bisogna sincronizzare host e device prima di prendere il tempo di fine:
\begin{minted}{c}
double iStart = cpuSecond();
kernel_name<<<grid, block>>>(argument list);
cudaDeviceSynchronize();
double iElaps = cpuSecond() - iStart;
\end{minted}

%End L2

\subsection{Warp}

\textbf{Ogni thread} vede: 
\begin{itemize}
	\item i suoi \textbf{registri privati}
	\item la \textbf{memoria condivisa} del blocco di thread
\end{itemize}

L'architettura SIMT (vedi \ref{par:simt}) si basa sugli \textbf{warp}, (tradotto in "\textit{trama}", termine che viene dalla tessitura), l'idea è che ci sono delle file di thread (warp), collegate assieme dall'ordito. Rappresenta i blocchi di thread, sono blocchi da 32. 

Ogni Streaming Mutiprocessor SM esegue i thread in gruppi di 32, chiamati warp. Idealmente, tutti i thread in un warp eseguono la stessa cosa in parallelo allo stesso tempo (SIMD all'interno del warp).

Ogni thread ha il suo program counter e register state di conseguenza può seguire cammini distinti di esecuzione delle istruzioni (parallelismo a livello thread, disponibile da Volta in poi, prima c'era un PC solo per ogni warp).

Il valore 32 è l'unità minima di esecuzione che permette grande efficienza nell'uso della GPU, concettualmente i blocchi di 32 dovrebbero avere modello SIMD, anche se nella pratica è SIMT (più flessibile ma potenzialmente meno efficiente). 

Dove si può si deve \textbf{evitare la divergenza di esecuzione} all'interno del warp. I \textbf{blocchi} vengono \textbf{divisi in warp}, quindi è meglio avere blocchi con thread multipli di 32, per evitare divergenza.

I blocchi di thread possono essere configurati logicamente in 1, 2 o 3 dimensioni, ma a livello hardware sarà una sola dimensione con id progressivo, con un warp ogni 32 thread.

Sarà quindi necessario uno scheduling per i warp (il numero di blocchi richiesto è maggiore, chi va prima in esecuzione?) all'interno dei blocchi, vengono mandati in esecuzione quando sono liberi. Ad ogni colpo di clock lo scheduler dei warp decide quale mandare in esecuzione tra quelli che
\begin{itemize}
	\item non sono in attesa di dati dalla device memory (alta latenza, memory latency)
	
	\item non stanno completando un'istruzione precedente (pipeline delay)
\end{itemize}

Questi dettagli sono trasparenti al programmatore, serve solo a garantire un elevato numero di warp in esecuzione; vogliamo massimizzare l'occupancy (percentuale di risorse usate in ogni SM).

Se all'interno di un warp dei thread devono eseguire istruzioni diverse (e.g., per colpa di un \texttt{if}), la GPU le eseguirà sequenzialmente al posto che in parallelo, disabilitando i thread inattivi. Questa è una \textbf{divergenza} ed ha impatto negativo sull'efficienza, a volte anche in maniera significativa. 

Ogni warp ha un contesto di esecuzione (runtime), trasparente al programmatore, che consta di: 
\begin{itemize}
	\item Program counters
	
	\item Registri a 32-bit ripartiti tra thread
	
	\item Shared memory ripartita tra blocchi
\end{itemize} 

Di conseguenza, la memoria locale ad ogni thread è limitata, bisogna prestare attenzione alle risorse richieste simultaneamente per ogni thread, altrimenti il numero totale di thread che possono essere attivi concorrentemente si riduce.

I registri sono usati per le variabili locali automatiche scalari (che non sono array quindi) e le coordinate dei thread. I dati nei registri sono privati ai thread (scope) e ogni multiprocessor ha un insieme di 32-bit register che sono partizionati tra i warp.

Il numero di blocchi e warp che possono essere elaborati insieme su un SM per un dato kernel dipende
\begin{itemize}
	\item dalla quantità di registri e di shared memory usata dal kernel
	
	\item dalla quantità di registri e shared memory resi disponibili dallo SM
\end{itemize}

Ogni architettura ha i suoi vincoli e noi vogliamo avvicinarci il più possibile ai limiti massimi, in modo da rendere il più efficiente possibile il programma. C'è un numero massimo di thread/blocchi/warp per SM, vogliamo fare in modo di avere l'utilizzo maggiore possibile.

\paragraph{Latency Hiding:} La "latenza" è il numero di cicli necessari al completamento di un'istruzione. Per massimizzare il throughput occorre che lo scheduler abbia sempre warp eleggibili a ogni ciclo di clock. Si ha così latency hiding scambiando la computazione tra warp.

Tipi di istruzioni che inducono latenza: 
\begin{itemize}
	\item Istruzioni aritmetiche: tempo necessario per la terminazione dell'operazione (\texttt{add}, \texttt{mult}, \dots); 10-20 cicli di clock
	
	\item Istruzioni di memoria: tempo necessario al dato per giungere a destinazione (\texttt{load}, \texttt{store}); 400-800 cicli di clock
\end{itemize}

La griglia viene suddivisa in blocchi, il blocco in thread, i blocchi vanno all'SM.
\begin{center}
	\includegraphics[width=0.8\linewidth]{img/cuda/modelandhwstruct}
\end{center}

\paragraph{Sincronizzazione a più livelli:} Le prestazioni decrescono con l'aumentare della divergenza nei warp. \textbf{Primitive di sincronizzazione} sono necessarie per evitare race conditions in cui diversi thread accedono simultaneamente alla stessa locazione di memoria. 

Si possono avere più livelli di sincronizzazione:
\begin{itemize}
	\item \textbf{System-level}: attesa che venga completato un dato task su entrambi host e device
	\begin{minted}{c}
cudaError_t cudaDeviceSynchronize(void);
	\end{minted}
	Blocca l'applicazione host finché tutte le operazioni CUDA non sono completate;
	
	\item \textbf{Block-level}: attesa che tutti i thread in un blocco raggiungano lo stesso punto di esecuzione
	\begin{minted}{c}
__device__ void __syncthreads(void);
	\end{minted}
	Sincronizza i thread all'interno di un blocco: attende fino a che tutti raggiungono il punto di sincronizzazione
	
	\item \textbf{Warp-level}: attesa che tutti i thread in un warp raggiungano lo stesso punto di esecuzione
	\begin{minted}{c}
__device__ void __syncwarp(mask);
	\end{minted}
	Sincronizza i thread all'interno di un warp: attende fino a che tutti raggiungono il punto di sincronizzazione (riconverge)
\end{itemize}

La sincronizzazione a livello di blocco va usata con attenzione, può anche portare a deadlock, un esempio semplice può essere una sincronizzazione dentro un \texttt{if-else}: potrebbero esserci thread che non entreranno mai nel ramo con la sincronizzazione, causando deadlock.

Il compilatore ha tecniche di ottimizzazione per evitare divergenza all'interno del warp (es: per un \texttt{if} calcola entrambi i branch).

%Manca par reduction s39-50?

\newpage

\subsection{Operazioni Atomiche}

Per evitare race conditions, le \textbf{operazioni atomiche} in CUDA eseguono (solo) operazioni matematiche senza interruzione da altri thread. Si tratta di funzioni che vengono tradotte in istruzioni singole.

Le operazioni basilari sono:
\begin{itemize}
	\item Matematiche: add, subtract, maximum, minimum, increment, and decrement
	
	\item Bitwise: AND, bitwise OR, bitwise XOR
	
	\item Swap: scambiano valore in memoria con uno nuovo
\end{itemize}

\subsection{Memoria CUDA}

Per il programmatore esistono due tipi di memorie: 
\begin{itemize}
	\item \textbf{Programmabile}: controllo esplicito di lettura e scrittura per dati che transitano in memoria
	
	\item \textbf{Non programmabile}: nessun controllo sull'allocazione dei dati, gestiti con tecniche automatiche (e.g., memorie CPU e cache L1 e L2 della GPU)
\end{itemize}

Nel modello di memoria CUDA sono esposti diversi tipi di memoria programmabile: 
\begin{enumerate}
	\item registri

	\item shared memory

	\item local memory

	\item constant memory

	\item texture memory

	\item global memory
\end{enumerate}

\paragraph{Cache su GPU:} Come nel caso delle CPU, le cache su GPU \textbf{non sono programmabili}. Sono presenti 4 tipi:
\begin{itemize}
	\item \textbf{L1}, una per ogni SM
	
	\item \textbf{L2}, condivisa tra tutti gli SM
	
	\item \textbf{Read-only constant}
	
	\item \textbf{Read-only texture} (L1 da CC 5.0)
\end{itemize}

La \textbf{cache L1} è presente all'interno di ogni SM; in alcune architetture (Fermi e successive) la dimensione può essere configurata, con una porzione assegnabile a memoria condivisa. Capacità limitata ma permette di sfruttare località dei dati.

La \textbf{cache L2} ha dimensione maggiore ed è condivisa tra tutti gli SM, funziona da intermediario tra memoria globale e cache L1 dei singoli SM. Raccoglie i dati necessari a tutti gli SM e contribuisce a mantenere la coerenza dei dati tra vari SM.

L1 e L2 sono usate per memorizzare dati in memoria locale e globale, incluso lo spilling dei registri (eccessi nell'uso di local memory).

Ogni SM ha anche una \textbf{read-only constant cache} e \textbf{read-only texture cache} (non sempre fisiche) usate per migliorare le prestazioni in lettura dai rispettivi spazi di memoria sul device.

Read-only constant cache è ottimizzata per dati globali costanti condivisi tra tutti i thread, con accesso uniforme e caching efficiente. Read-only texture cache è ideale per dati in sola lettura con accesso non coalescente, sfruttando la località spaziale e offrendo funzionalità di interpolazione e filtraggio hardware.

Suddivisione fisica:
\begin{center}
	\includegraphics[width=0.85\linewidth]{img/cuda/mem1}
\end{center}

Nel tempo, è stata gradualmente aumentata la dimensione delle cache L1 e L2, allo stesso tempo incrementando la memoria condivisa tra gli SM, fino all'introduzione di una L0 instruction cache in Volta, oltre a 128kB di cache L1 unita alla shared memory (smem).

\subsubsection{Cooperating Threads/Shared Memory}

Un blocco può avere della \textbf{memoria condivisa} e tutti thread all'interno del blocco hanno la stessa visuale su questa memoria; la memoria è unica per blocco e inaccessibile ad altri blocchi. Viene dichiarata tramite \texttt{\_\_shared\_\_}.

La SMEM è suddivisa in moduli della stessa ampiezza, chiamati \textbf{bank}. Ogni richiesta di accesso fatta di $n$ indirizzi che riguardano $n$ distinti bank sono serviti simultaneamente.

Ogni SM ha una quantità limitata di shared memory che viene ripartita tra i blocchi di thread. La smem serve come base per la comunicazione inter-thread: i thread all'interno di un blocco possono cooperare scambiandosi dati memorizzati in shared memory. L'accesso deve essere sincronizzato per mezzo di \texttt{syncthreads()}.

\paragraph{Organizzazione fisica:} La smem è suddivisa in blocchi da 4 byte (word), ogni accesso legge almeno la word di appartenenza (anche se viene richiesto un solo byte).

Dati 32 bank, ogni word è memorizzata in bank distinti, a gruppi di 32. Dato l'indirizzo del byte: 
\begin{itemize}
	\item diviso 4 si ottiene l'indice della word

	\item l'indice della word modulo $32$ è l'indice della bank
\end{itemize}

\paragraph{Smem a runtime:} La memoria viene ripartita tra tutti i blocchi residenti in un SM. Maggiore è la shared memory richiesta da un kernel, minore è il numero di blocchi attivi concorrenti. 

Il contenuto della shared memory ha lo stesso lifetime del blocco a cui è stata assegnata.

\paragraph{Pattern di accesso:} Se un'operazione di \texttt{load} o \texttt{store} eseguita da un warp richiede al più un accesso per bank, si può effettuare in una sola transizione il trasferimento dei dati dalla shared memory al warp. In alternativa sono richieste diverse ($\leq 32$) transazioni, con effetti negativi sulla bandwidth globale.

L'accesso ideale è una singola transazione per warp.

Ci possono essere dei \textbf{conflitti}: un \textbf{bank conflict} accade quando si hanno diversi indirizzi di shared memory che insistono sullo stesso bank.

L'hardware effettua tante transazioni quante ne sono necessarie per eliminare i conflitti, diminuendo la bandwidth effettiva di un fattore pari al numero di transazioni separate necessarie (vengono serializzati gli accessi).

\paragraph{Osservazioni:}
\begin{itemize}
	\item \textbf{Latency hiding}: il ritardo tra richiesta dei thread alla smem e l'ottenimento dei dati, in generale, non è un problema, anche in caso di bank conflict; lo scheduler passa a un altro warp in attesa che quelli sospesi completino il trasferimento dei dati dalla smem
	
	\item \textbf{Inter-block}: non esiste conflitto tra thread appartenenti a blocchi differenti, il problema sussiste solo a livello di warp dello stesso blocco 
	
	\item \textbf{Efficienza massima}: il modo più semplice per avere prestazioni elevate è quello di fare in modo che un warp acceda a word consecutive in memoria shared
	
	\item \textbf{Caching}: con scheduling efficace, le prestazioni (anche in presenza di conflitti a livello smem) sono molto migliori rispetto alla cache L2 o global memory
\end{itemize}

\subsubsection{Allocazione della SMEM}

\paragraph{Allocazione statica:} Una variabile in shared memory può anche essere dichiarata locale a un kernel o globale in un file sorgente. Viene dichiarata con il qualificatore \texttt{\_\_shared\_\_}. Può essere dichiarata sia staticamente sia dinamicamente. 

Se statica, può essere 1D, 2D o 3D, con dimensione nota compile time.

\paragraph{Allocazione dinamica:} Per allocare la shared memory dinamicamente (in bytes), occorre indicare un terzo argomento all'interno della chiamata del kernel:
\begin{minted}{c}
kernel <<<grid, block, N*sizeof(int)>>>(...)
\end{minted}

Se la dimensione non è nota compile time, è possibile dichiarare una variabile adimensionale con la keyword \texttt{extern}. Dinamicamente si possono allocare solo array 1D.

\paragraph{Allocazione dinamica multipla:} Non si possono allocare multiple allocazioni di shared memory, quindi bisogna fare un'allocazione unica e utilizzare puntatori con offset all'interno dell'area allocata.

Uso tipico della shared memory:
\begin{enumerate}
	\item Carica i dati dalla device memory alla shared memory
	
	\item Sincronizza i thread del blocco al termine della copia che ognuno effettua sulla shared memory (così che ogni thread possa elaborare dati certi andando avanti)
	
	\item Elabora i dati in shared memory

	\item Sincronizza (se necessario) per essere certi che la shared memory contenga i risultati aggiornati
	
	\item Scrivi i risultati dalla device memory alla host memory
\end{enumerate}

\subsubsection{Prodotto Convolutivo con SMEM}
La \href{https://it.wikipedia.org/wiki/Convoluzione}{\texttt{convoluzione}} sono una serie di somme e prodotti
$$ y(n) = h(n) \cdot x(n) = \sum_{k=0}^{N-1} h(k) x(n-k) $$

Si prende un segnale, si considera un kernel/finestra su tale segnale, si fanno i prodotti. Il tutto viene fatto un numero \textit{molto elevato} di volte. Si può scalare il processo a più dimensioni.

Ci sono sempre problemi di bordo: cosa faccio quando la maschera considerata arriva al bordo dei dati? Andrebbe fuori, quindi devo popolare nel modo corretto i dati mancanti (\texttt{0}? Li invento?).

\paragraph{Tiling:} Divido i dati in blocchi (ad esempio, 16 elementi in 4 blocchi da 4 thread), ogni thread nel blocco fa un prodotto della convoluzione. Per ridurre l'accesso alla global memory, in cache/memoria condivisa si tengono i dati a cui l'accesso è fatto più frequentemente, ovvero i valori del "blocco di dati" assegnato al block (tutti i thread devono calcolare sullo stesso insieme di dati, o quasi), tenendo conto della dimensione della maschera (serve avere i dati "adiacenti" al blocco (sarebbe molto utile un'immagine, si capirebbe subito)).

I dati da caricare in smem sono più dei thread nel blocco ("alone" che va al di fuori del blocco di dati stesso); in smem carico tutti i possibili dati a cui il blocco deve fare l'accesso. Chi carica che dati in memoria? I dati esterni al blocco potrebbero essere anche più del blocco stesso (maschera "grossa"). La soluzione è dare un ordine ai thread e dividere il più equamente possibile i caricamenti in memoria tra i thread del blocco.

\newpage

\subsection{Global Memory}

Nei computer moderni esiste una gerarchia di memorie per minimizzare latenze e massimizzare throughput. In genere, si ha l'illusione virtuale di una grande memoria, tutta a bassa latenza, anche se la memoria con effettivamente bassa latenza è poca e si ha una memoria ad alta capacità e alta latenza.

All'interno delle GPU abbiamo, dalla latenza più alta alla più bassa: 
\begin{itemize}
	\item Device Memory
	
	\item L2 Cache
	
	\item L1/shared
	
	\item Registers
\end{itemize}

Le gerarchie di memorie, comprese quelle CUDA, fanno fede ai principi di: 
\begin{itemize}
	\item \textbf{Località spaziale}: se l'istruzione all'indirizzo $i$ viene eseguita, probabilmente dopo verrà eseguita quella all'indirizzo $i + \Delta i$
	
	\item \textbf{Località temporale}: se un'istruzione viene eseguita al tempo $t$, probabilmente verrà eseguita anche al tempo $t + \Delta t$ (dove $\Delta t$ è piccolo)
\end{itemize}

\paragraph{Registers:} Le memorie più veloci in assoluto, con lifetime del kernel. Vengono ripartiti tra i warp attivi, le variabili dichiarate nel codice device senza qualificatori generalmente risiedono in un registro. 

Meno registri usa il kernel, più blocchi di thread è probabile che risiedano sull'SM (il compilatore usa un'euristica per ottimizzare questo parametro). \textbf{Register spilling}: se si usano più registri di quelli consentiti le variabili si riversano nella local memory.

\paragraph{Local Memory:} Si tratta di una memoria \textit{lenta} (collocata off-chip, alta latenza, bassa bandwidth). Si tratta di una memoria locale ai thread.

Usata per contenere le variabili automatiche (grandi) non contenute nei registri, o per le variabili al di fuori causa spilling. La local memory risiede nella device memory, pertanto gli accessi hanno stessa latenza e ampiezza di banda della global memory e sono soggetti anche agli stessi vincoli di coalescenza-

Da CC 2.0 ci sono parti poste in cache L1 a livello di SM e in cache L2 a livello di device. Il compilatore \texttt{nvcc} si preoccupa della sua allocazione e non è controllata dal programmatore.

\paragraph{Constant Memory:} Risiede nella device memory (64K per tutte le CC) ed ha una cache dedicata in ogni SM (8K). Definibile tramite l'attributo \texttt{\_\_constant\_\_}. 

Ospita dati in sola lettura, ideale per accessi uniformi. Ha scope globale, va dichiarata al di fuori di qualsiasi kernel e viene dichiarata staticamente, quindi è visibile a tutti i kernel nella stessa unità di compilazione.

La constant memory può essere inizializzata dall'host usando:
\begin{minted}{c}
cudaError_t cudaMemcpyToSymbol(const void* symbol,
    const void* src, size_t count)
\end{minted}

Lavora bene quando tutti i thread di un warp leggono dallo stesso indirizzo di memoria (raggiunge l'efficienza dei registri); se i thread di un warp leggono da indirizzi diversi allora le letture vengono serializzate, riducendo l'efficienza.

\paragraph{Texture Memory:} Risiede nella device memory e (può avere) una read-only cache per-SM ed è acceduta solo attraverso di essa. La cache include un supporto hardware efficiente per filtraggio o interpolazione floating-point nel processo di lettura dei dati. 

Ottimizzata per la località spaziale 2D, quindi dati espressi sotto forma di matrici. I thread in un warp che usano la texture memory per accedere a dati 2D hanno migliori prestazioni rispetto a quelle standard, quindi è adatta per applicazioni in cui servono classiche elaborazioni di immagini/video. Per altre applicazioni l’uso della texture memory potrebbe essere più lento della global memory.

\paragraph{Global Memory:} La più grande, con più alta latenza e più comunemente usata memoria su GPU. Ha scope e lifetime globale (da qui il nome). Dichiarazione (codice host):
\begin{center}
	\begin{tabular}{r | l r | }
		\cline{2-3}
		\textbf{Statica} & \texttt{\_\_device\_\_ int a[N];} & \\
		\textbf{Dinamica} & \texttt{cudaMalloc((void **)\&d\_a, N);} & \texttt{cudaFree(d\_a);} \\
		\cline{2-3}
	\end{tabular}
\end{center}

Corrisponde alla memoria fisica, con "global" si intende una divisione logica. L'accesso da parte di thread appartenenti a blocchi distinti può potenzialmente portare a modifiche incoerenti. La global memory è accessibile attraverso transazioni da 32, 64, o 128 byte; le transazioni avvengono solo per gruppi di valori, non si può accedere a un valore singolo.

I valori contenuti nella memoria allocata non sono inizializzati, ma si possono inizializzare con dati provenienti dall'host (\texttt{cudaMemcpy}) oppure con un valore specifico
\begin{minted}{c}
cudaError_t cudaMemset(void* devPtr, int value, size_t count)
\end{minted}

Assegna il valore \texttt{value} a tutti gli indirizzi contenuti nel blocco di memoria.

La memoria allocata è opportunamente allineata per ogni tipo di variabile. La \texttt{cudaMalloc} restituisce \texttt{cudaErrorMemoryAllocation} in caso di fallimento.

Lo \textbf{specificatore \texttt{\_\_device\_\_}} indica una variabile che risiede unicamente sul device. Risiede nella memoria globale (e quindi oggetti distinti per device diversi), ha il lifetime del contesto CUDA in cui è stata creata. Può essere acceduta da tutti i thread e dall'host tramite la libreria runtime:
\begin{itemize}
	\item \texttt{cudaGetSymbolAddress()}, \texttt{cudaGetSymbolSize()}: per ottenere indirizzo e dimensione di una variabile, rispettivamente
	
    \item \texttt{cudaMemcpyToSymbol()}, \texttt{cudaMemcpyFromSymbol()}: per copiare verso e da una variabile, rispettivamente
\end{itemize}

Riassunto dichiarazione di variabili: 
\begin{center}
	\begin{tabular}{|l|l|l|l|l|}
		\hline
		\textbf{QUALIFIER} & \textbf{VARIABLE} & \textbf{MEMORY} & \textbf{SCOPE} & \textbf{LIFESPAN} \\
		\hline
		& \texttt{float var} & Register & Local & Thread \\
		\hline
		& \texttt{float var[100]} & Local & Local & Thread \\
		\hline
		\texttt{\_\_shared\_\_} & \texttt{float var} & Shared & Block & Block \\
		\hline
		\texttt{\_\_device\_\_} & \texttt{float var} & Global & Global & Application \\
		\hline
		\texttt{\_\_constant\_\_} & \texttt{float var} & Constant & Global & Application \\
		\hline
	\end{tabular}
\end{center}

\subsection{Pinned memory}
La pinned memory (o page-locked memory) in CUDA è una tecnica che serve per ottimizzare il trasferimento dei dati tra la memoria del sistema (RAM) e la memoria della GPU (VRAM). 

Si vuole evitare il page fault della virtual memory (CPU, di default la memoria host allocata è paginabile). Esistono delle primitive per definire una memoria pinned, ovvero viene tolta la pagina dal meccanismo di virtualizzazione in modo che l'host non possa "toglierla" mentre il device la deve usare. Blocca la memoria in modo da poter fare trasferimenti asincroni al device.

Una volta "pinnata", la memoria non sparirà dal sistema di virtualizzazione automatico della memoria host, quindi si può lavorare su quella memoria in maniera asincrona. La pinned memory può essere acceduta direttamente dal device, in modalità asincrona. Può essere letta e scritta con una bandwidth più alta rispetto alla memoria paginabile.

Da notare che eccessi di allocazione di pinned memory potrebbero far degradare le prestazioni dell'host (ridurre la memoria paginabile inficia l'uso della virtual memory), Per allocare esplicitamente memoria pinned:
\begin{minted}{c}
cudaError_t cudaMallocHost(void **devPtr, size_t count);
\end{minted}

E per deallocarla:
\begin{minted}{c}
cudaError_t cudaFreeHost(void *devPtr);
\end{minted}

Questa allocazione sostituisce la malloc "normale", su host. Rende i trasferimenti host-device significativamente più veloci, al costo di un tempo più alto di allocazione.

\subsection{Unified Virtual Addressing UVA}

Si vuole avere un unico spazio di indirizzamento tra CPU e tutte le GPU. Tutti i puntatori (CPU e GPU) appartengono allo stesso spazio di indirizzi virtuali, di conseguenza è possibile passare un puntatore da host a device e viceversa senza ambiguità, entrambi possono "capire" a cosa punta quell'indirizzo.

% End L5

La unified memory è una memoria "\textit{comoda}", fornisce un puntatore unico per tutte le CPU e GPU presenti nel sistema. Spazio di indirizzamento unico per CPU e GPU.

Con "\textbf{Managed Memory}" si fa riferimento ad allocazioni della unified memory. All'interno di un kernel si possono usare entrambi i tipi di memoria: 
\begin{itemize}
	\item managed memory, controllata dal sistema
    
	\item un-managed memory, controllata esplicitamente dall'applicazione
\end{itemize}

Tutte le operazioni CUDA valide sulla memoria del dispositivo sono valide anche sulla memoria managed.

Per fare allocazione dinamica:
\begin{minted}{c}
cudaError_t cudaMallocManaged(void **devPtr, size_t size, 
    unsigned int flags=0)
\end{minted}

"rimpiazza" \texttt{cudaMalloc}, la \texttt{flag} indica chi condivide il puntatore con il device:
\begin{itemize}
	\item \texttt{cudaMemAttachHost}: solo la CPU
    
	\item \texttt{cudaMemAttachGlobal}: anche tutte le altre GPU
\end{itemize}

Nuova \textbf{keyword}: \textbf{\texttt{\_\_managed\_\_}}, si tratta di un qualifier che denota scope globale, accessibile da CPU e GPU.

Con l'uso "misto" di memoria bisogna porre attenzione alla sincronizzazione tra CPU e GPU, onde evitare problemi.

\subsection{Pattern di Accesso alla Global Memory}

Gli accessi alla memoria del dispositivo possono avvenire in transazioni da 32, 64 o 128 byte. Le applicazioni GPU tendono (a volte) ad essere limitate dalla memory bandwidth, quindi massimizzare il throughput effettivo è importante. 

In generale, per rendere efficienti le transazioni in memoria:
\begin{itemize}
	\item minimizzare il numero di transazioni per servire il massimo numero di accessi
    
	\item considerare che il numero di transazioni e throughput ottenuto variano con la CC
\end{itemize}

Per migliorare le prestazioni in lettura e scrittura occorre ricordare che: 
\begin{itemize}
	\item le istruzioni vengono eseguite a livello di warp e gli accessi in memoria dipendono dalle operazioni svolte nel warp
    
	\item per un dato indirizzo si esegue un'operazione di loading o storing (gestione diversa)
	
    \item i 32 thread presentano una singola richiesta di accesso, che viene servita da una o più transazioni in memoria
\end{itemize}

In base a come sono distribuiti gli indirizzi di memoria, gli accessi alla stessa possono essere classificati in pattern distinti. Tutti gli accessi a memoria globale passano dalla cache L2, molti passano anche dalla L1. Se entrambe le memorie vengono usate gli accessi sono da 128 byte, altrimenti, se viene usata solo la L2, gli accessi sono a 32 byte.

Per le architetture che usano cache L1, queste possono essere esplicitamente abilitate o disabilitate a compile time.

Bisogna rispettare allineamento e coalescenza per sfruttare al meglio le transazioni di memoria; per avere accessi in memoria efficienti è necessario combinare in un unica transazione accessi multipli a memoria allineati e coalescenti.

Accesso \textbf{allineato}: quando il primo indirizzo della transazione è un multiplo pari della granularità della cache che viene usata per servire la transazione (32 byte per la cache L2 o 128 byte per la cache L1).

Accesso \textbf{coalescente:} quando tutti i 32 thread in un warp accedono a un blocco contiguo di memoria.

In un SM i dati seguono pipeline attraverso i seguenti tre cache/buffer paths dipendentemente da quale tipo di device memory si accede:
\begin{itemize}
	\item L1/L2 cache
    
	\item Constant cache
	
    \item Read-only cache
\end{itemize}

L1/L2 cache rappresenta il default path. Il fatto che un'operazione di \texttt{load} in global memory passi attraverso la cache L1 dipende da CC e compiler options.

\paragraph{Scritture:} Le write vengono servite in modo diverso, non viene usata la cache L1, ma le \texttt{store} sono cachate solo in L2, prima di essere inviate alla device memory in segmenti da 32 byte; vengono trasferiti 1,2 o 4 segmenti alla volta.

Quando forzati a fare accessi (letture/scritture) non coalescenti si può usare la shared memory come "passaggio" per rendere le operazioni effettive in memoria coalescenti.